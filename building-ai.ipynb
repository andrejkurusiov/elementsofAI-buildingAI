{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.0 64-bit ('elementsofAI-buildingAI')",
   "metadata": {
    "interpreter": {
     "hash": "f4ec3d2c758103b3d82cbd9bbd8191816dab5ba84e762f9c456c6de795c8fdec"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# [Elements of AI: Building AI](https://buildingai.elementsofai.com/)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Getting started with AI"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Requirement already satisfied: numpy in /Users/ak/.pyenv/versions/3.9.0/envs/elementsofAI-buildingAI/lib/python3.9/site-packages (1.20.1)\n",
      "Requirement already satisfied: scikit-learn in /Users/ak/.pyenv/versions/3.9.0/envs/elementsofAI-buildingAI/lib/python3.9/site-packages (0.24.1)\n",
      "Requirement already satisfied: scipy>=0.19.1 in /Users/ak/.pyenv/versions/3.9.0/envs/elementsofAI-buildingAI/lib/python3.9/site-packages (from scikit-learn) (1.6.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/ak/.pyenv/versions/3.9.0/envs/elementsofAI-buildingAI/lib/python3.9/site-packages (from scikit-learn) (2.1.0)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /Users/ak/.pyenv/versions/3.9.0/envs/elementsofAI-buildingAI/lib/python3.9/site-packages (from scikit-learn) (1.20.1)\n",
      "Requirement already satisfied: joblib>=0.11 in /Users/ak/.pyenv/versions/3.9.0/envs/elementsofAI-buildingAI/lib/python3.9/site-packages (from scikit-learn) (1.0.1)\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.Javascript object>",
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 30;\n                var nbb_unformatted_code = \"# Install necessary packages withing the current environment\\n!python -m pip install numpy\\n!python -m pip install -U scikit-learn\";\n                var nbb_formatted_code = \"# Install necessary packages withing the current environment\\n!python -m pip install numpy\\n!python -m pip install -U scikit-learn\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            "
     },
     "metadata": {}
    }
   ],
   "source": [
    "# Install necessary packages withing the current environment\n",
    "!python -m pip install numpy\n",
    "!python -m pip install -U scikit-learn"
   ]
  },
  {
   "source": [
    "## II.Optimization\n",
    "\n",
    "### Exercise 1: Listing pineapple routes\n",
    "\n",
    "###  -- Advanced\n",
    "\n",
    "Imagine that you've been assigned the task to plan the route of a container ship loaded with pineapples. The ship starts in Panama, loaded with delicious Fairtrade pineapples. There are four other ports, New York, Casablanca, Amsterdam, and Helsinki, where pineapple-craving citizens are eagerly waiting. The ship must visit each of the four destination ports exactly once, but the order in which each port is visited is free. The goal is to minimize the carbon emissions, which means that a shorter route is better than a longer one.\n",
    "\n",
    "To solve this problem, it is enough to list all the possible routes that start from Panama and visit each of the other ports exactly once, calculate the carbon emissions of each route, and print out the one with the least emissions.\n",
    "\n",
    "Before we try to find the optimal route, let's start by listing all the alternative routes. After all, it wouldn't make sense to stop at any port more than once.\n",
    "\n",
    "Write a program that takes a list (in this case, the names of the ports) and prints out all the possible orderings of them. The mathematical term for such orderings is a permutation. Note that your program should work for an input list of any length. The order in which the permutations are printed doesn't matter, but they should all begin with Panama (PAN).\n",
    "\n",
    "The format of the output should be such that each permutation is printed on its own row as one string with the port names separated by spaces. You can use the join function as follows: `print(' '.join([portnames[i] for i in route]))`."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "PAN AMS CAS NYC HEL\nPAN AMS CAS HEL NYC\nPAN AMS NYC CAS HEL\nPAN AMS NYC HEL CAS\nPAN AMS HEL CAS NYC\nPAN AMS HEL NYC CAS\nPAN CAS AMS NYC HEL\nPAN CAS AMS HEL NYC\nPAN CAS NYC AMS HEL\nPAN CAS NYC HEL AMS\nPAN CAS HEL AMS NYC\nPAN CAS HEL NYC AMS\nPAN NYC AMS CAS HEL\nPAN NYC AMS HEL CAS\nPAN NYC CAS AMS HEL\nPAN NYC CAS HEL AMS\nPAN NYC HEL AMS CAS\nPAN NYC HEL CAS AMS\nPAN HEL AMS CAS NYC\nPAN HEL AMS NYC CAS\nPAN HEL CAS AMS NYC\nPAN HEL CAS NYC AMS\nPAN HEL NYC AMS CAS\nPAN HEL NYC CAS AMS\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.Javascript object>",
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 31;\n                var nbb_unformatted_code = \"from itertools import permutations as perm\\nportnames = [\\\"PAN\\\", \\\"AMS\\\", \\\"CAS\\\", \\\"NYC\\\", \\\"HEL\\\"]\\n \\ndef permutations(route, ports):\\n    for route in perm(ports):\\n        print('PAN ' + ' '.join([portnames[i] for i in route]))\\n\\n# this will start the recursion with 0 as the first stop\\npermutations([0], list(range(1, len(portnames))))\";\n                var nbb_formatted_code = \"from itertools import permutations as perm\\n\\nportnames = [\\\"PAN\\\", \\\"AMS\\\", \\\"CAS\\\", \\\"NYC\\\", \\\"HEL\\\"]\\n\\n\\ndef permutations(route, ports):\\n    for route in perm(ports):\\n        print(\\\"PAN \\\" + \\\" \\\".join([portnames[i] for i in route]))\\n\\n\\n# this will start the recursion with 0 as the first stop\\npermutations([0], list(range(1, len(portnames))))\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            "
     },
     "metadata": {}
    }
   ],
   "source": [
    "from itertools import permutations as perm\n",
    "\n",
    "portnames = [\"PAN\", \"AMS\", \"CAS\", \"NYC\", \"HEL\"]\n",
    "\n",
    "\n",
    "def permutations(route, ports):\n",
    "    for route in perm(ports):\n",
    "        print(\"PAN \" + \" \".join([portnames[i] for i in route]))\n",
    "\n",
    "\n",
    "# this will start the recursion with 0 as the first stop\n",
    "permutations([0], list(range(1, len(portnames))))"
   ]
  },
  {
   "source": [
    "### Exercise 2: Pineapple route emissions\n",
    "\n",
    "### -- Advanced\n",
    "\n",
    "Having listed the alternatives, next we can calculate the carbon emissions for each of them.\n",
    "Modify the code so that it finds the route with minimum carbon emissions and prints it out. Again, the program should work for any number of ports. You can assume that the distances between the ports are given in an array of the appropriate size so that the distance between ports i and j is found in `D[i][j]`."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "PAN NYC CAS AMS HEL 283.7 kg\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.Javascript object>",
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 32;\n                var nbb_unformatted_code = \"from itertools import permutations as perm\\n\\nportnames = [\\\"PAN\\\", \\\"AMS\\\", \\\"CAS\\\", \\\"NYC\\\", \\\"HEL\\\"]\\n\\n# https://sea-distances.org/\\n# nautical miles converted to km\\n\\nD = [\\n        [0,8943,8019,3652,10545],\\n        [8943,0,2619,6317,2078],\\n        [8019,2619,0,5836,4939],\\n        [3652,6317,5836,0,7825],\\n        [10545,2078,4939,7825,0]\\n    ]\\n\\n# https://timeforchange.org/co2-emissions-shipping-goods\\n# assume 20g per km per metric ton (of pineapples)\\n\\nco2 = 0.020\\n\\n# DATA BLOCK ENDS\\n\\n# these variables are initialised to nonsensical values\\n# your program should determine the correct values for them\\nsmallest = 1000000\\nbestroute = [0, 0, 0, 0, 0]\\n\\ndef permutations(route, ports):\\n    global smallest, bestroute\\n\\n    for r in perm(ports):\\n        r = route + list(r)\\n        # em = co2 * (D[r[0]][r[1]] + D[r[1]][r[2]] + D[r[2]][r[3]] + D[r[3]][r[4]])\\n        em = co2 * sum(D[i][j] for i, j in zip(r[:-1], r[1:]))\\n        if em < smallest:\\n            smallest = em\\n            bestroute = r\\n\\ndef main():\\n    # this will start the recursion \\n    permutations([0], list(range(1, len(portnames))))\\n\\n    # print the best route and its emissions\\n    print(' '.join([portnames[i] for i in bestroute]) + \\\" %.1f kg\\\" % smallest)\\n\\nmain()\";\n                var nbb_formatted_code = \"from itertools import permutations as perm\\n\\nportnames = [\\\"PAN\\\", \\\"AMS\\\", \\\"CAS\\\", \\\"NYC\\\", \\\"HEL\\\"]\\n\\n# https://sea-distances.org/\\n# nautical miles converted to km\\n\\nD = [\\n    [0, 8943, 8019, 3652, 10545],\\n    [8943, 0, 2619, 6317, 2078],\\n    [8019, 2619, 0, 5836, 4939],\\n    [3652, 6317, 5836, 0, 7825],\\n    [10545, 2078, 4939, 7825, 0],\\n]\\n\\n# https://timeforchange.org/co2-emissions-shipping-goods\\n# assume 20g per km per metric ton (of pineapples)\\n\\nco2 = 0.020\\n\\n# DATA BLOCK ENDS\\n\\n# these variables are initialised to nonsensical values\\n# your program should determine the correct values for them\\nsmallest = 1000000\\nbestroute = [0, 0, 0, 0, 0]\\n\\n\\ndef permutations(route, ports):\\n    global smallest, bestroute\\n\\n    for r in perm(ports):\\n        r = route + list(r)\\n        # em = co2 * (D[r[0]][r[1]] + D[r[1]][r[2]] + D[r[2]][r[3]] + D[r[3]][r[4]])\\n        em = co2 * sum(D[i][j] for i, j in zip(r[:-1], r[1:]))\\n        if em < smallest:\\n            smallest = em\\n            bestroute = r\\n\\n\\ndef main():\\n    # this will start the recursion\\n    permutations([0], list(range(1, len(portnames))))\\n\\n    # print the best route and its emissions\\n    print(\\\" \\\".join([portnames[i] for i in bestroute]) + \\\" %.1f kg\\\" % smallest)\\n\\n\\nmain()\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            "
     },
     "metadata": {}
    }
   ],
   "source": [
    "from itertools import permutations as perm\n",
    "\n",
    "portnames = [\"PAN\", \"AMS\", \"CAS\", \"NYC\", \"HEL\"]\n",
    "\n",
    "# https://sea-distances.org/\n",
    "# nautical miles converted to km\n",
    "\n",
    "D = [\n",
    "    [0, 8943, 8019, 3652, 10545],\n",
    "    [8943, 0, 2619, 6317, 2078],\n",
    "    [8019, 2619, 0, 5836, 4939],\n",
    "    [3652, 6317, 5836, 0, 7825],\n",
    "    [10545, 2078, 4939, 7825, 0],\n",
    "]\n",
    "\n",
    "# https://timeforchange.org/co2-emissions-shipping-goods\n",
    "# assume 20g per km per metric ton (of pineapples)\n",
    "\n",
    "co2 = 0.020\n",
    "\n",
    "# DATA BLOCK ENDS\n",
    "\n",
    "# these variables are initialised to nonsensical values\n",
    "# your program should determine the correct values for them\n",
    "smallest = 1000000\n",
    "bestroute = [0, 0, 0, 0, 0]\n",
    "\n",
    "\n",
    "def permutations(route, ports):\n",
    "    global smallest, bestroute\n",
    "\n",
    "    for r in perm(ports):\n",
    "        r = route + list(r)\n",
    "        # em = co2 * (D[r[0]][r[1]] + D[r[1]][r[2]] + D[r[2]][r[3]] + D[r[3]][r[4]])\n",
    "        em = co2 * sum(D[i][j] for i, j in zip(r[:-1], r[1:]))\n",
    "        if em < smallest:\n",
    "            smallest = em\n",
    "            bestroute = r\n",
    "\n",
    "\n",
    "def main():\n",
    "    # this will start the recursion\n",
    "    permutations([0], list(range(1, len(portnames))))\n",
    "\n",
    "    # print the best route and its emissions\n",
    "    print(\" \".join([portnames[i] for i in bestroute]) + \" %.1f kg\" % smallest)\n",
    "\n",
    "\n",
    "main()"
   ]
  },
  {
   "source": [
    "## III.Hill climbing\n",
    "\n",
    "### Exercise 3: Reach the highest summit\n",
    "\n",
    "### -- Advanced\n",
    "\n",
    "Let the elevation at each point on the mountain be stored in array h of size 100. The elevation at the leftmost point is thus stored in `h[0]` and the elevation at the rightmost point is stored in `h[99]`.\n",
    "\n",
    "The following program starts at a random position and keeps going to the right until Venla can no longer go up. However, perhaps the mountain is a bit rugged which means it's necessary to look a bit further ahead.\n",
    "\n",
    "Edit the program so that Venla doesn't stop climbing as long as she can go up by moving up to five steps either left or right. If there are multiple choices within five steps that go up, any one of them is good. To check how your climbing algorithm works in action, you can plot the results of your hill climbing using the Plot button. The summit will be marked with a blue triangle."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(81, 72)"
      ]
     },
     "metadata": {},
     "execution_count": 33
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.Javascript object>",
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 33;\n                var nbb_unformatted_code = \"import math\\nimport random             \\t# just for generating random mountains                                 \\t \\n\\n# generate random mountains\\nw = [.05, random.random()/3, random.random()/3]\\nh = [1.+math.sin(1+x/.6)*w[0]+math.sin(-.3+x/9.)*w[1]+math.sin(-.2+x/30.)*w[2] for x in range(100)]\\n\\ndef climb(x, h):\\n    # keep climbing until we've found a summit\\n    summit = False\\n    steps_max = 5    # range to check\\n\\n    # Edit the program so that Venla doesn't stop climbing as long as she can go up by moving up to five steps either left or right.\\n    while not summit:\\n        summit = True\\n        for x_new in range(max(0, x - steps_max), min(99, x + steps_max)):\\n            if h[x_new] > h[x]:\\n                x = x_new         # here is higher, go here \\n                summit = False    # and keep going\\n    return x\\n\\ndef main(h):\\n    # start at a random place                                                                                  \\t \\n    x0 = random.randint(1, 98)\\n    x = climb(x0, h)\\n\\n    return x0, x\\n\\nmain(h)\";\n                var nbb_formatted_code = \"import math\\nimport random  # just for generating random mountains\\n\\n# generate random mountains\\nw = [0.05, random.random() / 3, random.random() / 3]\\nh = [\\n    1.0\\n    + math.sin(1 + x / 0.6) * w[0]\\n    + math.sin(-0.3 + x / 9.0) * w[1]\\n    + math.sin(-0.2 + x / 30.0) * w[2]\\n    for x in range(100)\\n]\\n\\n\\ndef climb(x, h):\\n    # keep climbing until we've found a summit\\n    summit = False\\n    steps_max = 5  # range to check\\n\\n    # Edit the program so that Venla doesn't stop climbing as long as she can go up by moving up to five steps either left or right.\\n    while not summit:\\n        summit = True\\n        for x_new in range(max(0, x - steps_max), min(99, x + steps_max)):\\n            if h[x_new] > h[x]:\\n                x = x_new  # here is higher, go here\\n                summit = False  # and keep going\\n    return x\\n\\n\\ndef main(h):\\n    # start at a random place\\n    x0 = random.randint(1, 98)\\n    x = climb(x0, h)\\n\\n    return x0, x\\n\\n\\nmain(h)\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            "
     },
     "metadata": {}
    }
   ],
   "source": [
    "import math\n",
    "import random  # just for generating random mountains\n",
    "\n",
    "# generate random mountains\n",
    "w = [0.05, random.random() / 3, random.random() / 3]\n",
    "h = [\n",
    "    1.0\n",
    "    + math.sin(1 + x / 0.6) * w[0]\n",
    "    + math.sin(-0.3 + x / 9.0) * w[1]\n",
    "    + math.sin(-0.2 + x / 30.0) * w[2]\n",
    "    for x in range(100)\n",
    "]\n",
    "\n",
    "\n",
    "def climb(x, h):\n",
    "    # keep climbing until we've found a summit\n",
    "    summit = False\n",
    "    steps_max = 5  # range to check\n",
    "\n",
    "    # Edit the program so that Venla doesn't stop climbing as long as she can go up by moving up to five steps either left or right.\n",
    "    while not summit:\n",
    "        summit = True\n",
    "        for x_new in range(max(0, x - steps_max), min(99, x + steps_max)):\n",
    "            if h[x_new] > h[x]:\n",
    "                x = x_new  # here is higher, go here\n",
    "                summit = False  # and keep going\n",
    "    return x\n",
    "\n",
    "\n",
    "def main(h):\n",
    "    # start at a random place\n",
    "    x0 = random.randint(1, 98)\n",
    "    x = climb(x0, h)\n",
    "\n",
    "    return x0, x\n",
    "\n",
    "\n",
    "main(h)"
   ]
  },
  {
   "source": [
    "### Exercise 4: Probabilities\n",
    "\n",
    "### -- Advanced\n",
    "\n",
    "Write a program that prints \"I love\" followed by one word: the additional word should be 'dogs' with 80% probability, 'cats' with 10% probability, and 'bats' with 10% probability."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "I love dogs\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.Javascript object>",
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 34;\n                var nbb_unformatted_code = \"import random\\n\\nx = random.random()\\nif x < 0.8:\\n    favourite = \\\"dogs\\\"\\nelif x < 0.9:\\n    favourite = \\\"cats\\\"\\nelse:\\n    favourite = \\\"bats\\\"\\n      \\nprint(\\\"I love\\\", favourite)\";\n                var nbb_formatted_code = \"import random\\n\\nx = random.random()\\nif x < 0.8:\\n    favourite = \\\"dogs\\\"\\nelif x < 0.9:\\n    favourite = \\\"cats\\\"\\nelse:\\n    favourite = \\\"bats\\\"\\n\\nprint(\\\"I love\\\", favourite)\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            "
     },
     "metadata": {}
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "x = random.random()\n",
    "if x < 0.8:\n",
    "    favourite = \"dogs\"\n",
    "elif x < 0.9:\n",
    "    favourite = \"cats\"\n",
    "else:\n",
    "    favourite = \"bats\"\n",
    "\n",
    "print(\"I love\", favourite)"
   ]
  },
  {
   "source": [
    "### Exercise 5: Warm-up Temperature\n",
    "\n",
    "### -- Advanced\n",
    "\n",
    "**Simulated Annealing: the math**\n",
    "\n",
    "The probability of accepting the new solution with score `S_new` when the current solution has score `S_old` is given by the formula:\n",
    "\n",
    "`prob = exp(–(S_old – S_new)÷T)`\n",
    "\n",
    "where T is the temperature. (Remember that the temperature is an abstract concept that ideally starts high and gradually decreases towards zero.) The function `exp(x)` is the exponent function which can also be written mathematically as `e^x` (the so called Euler's constant e ≅ 2.71828 raised to power x).\n",
    "\n",
    "Suppose the current solution has score S_old = 150 and you try a small modification to create a new solution with score S_new = 140. In the greedy solution, this new solution wouldn't be accepted because it would mean a decrease in the score. In simulated annealing, the new solution is accepted with a certain probability as explained above.\n",
    "\n",
    "Modify the accept_prob function so that it returns the probability of accepting the new state using simulated annealing. The program should take the two score values (the current and the new) and the temperature value as arguments."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.Javascript object>",
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 35;\n                var nbb_unformatted_code = \"import random\\nfrom numpy import exp\\n#from math import e\\n\\ndef accept_prob(S_old, S_new, T):\\n    # this is the acceptance \\\"probability\\\" in the greedy hill-climbing method\\n    # where new solutions are accepted if and only if they are better\\n    # than the old one.\\n    # change it to be the acceptance probability in simulated annealing\\n    return 1.0 if S_new > S_old else exp(-(S_old - S_new)/T)\\n\\n\\n# the above function will be used as follows. this is shown just for\\n# your information; you don't have to change anything here\\ndef accept(S_old, S_new, T):\\n    if random.random() < accept_prob(S_old, S_new, T):\\n        print(True)\\n    else:\\n        print(False)\";\n                var nbb_formatted_code = \"import random\\nfrom numpy import exp\\n\\n# from math import e\\n\\n\\ndef accept_prob(S_old, S_new, T):\\n    # this is the acceptance \\\"probability\\\" in the greedy hill-climbing method\\n    # where new solutions are accepted if and only if they are better\\n    # than the old one.\\n    # change it to be the acceptance probability in simulated annealing\\n    return 1.0 if S_new > S_old else exp(-(S_old - S_new) / T)\\n\\n\\n# the above function will be used as follows. this is shown just for\\n# your information; you don't have to change anything here\\ndef accept(S_old, S_new, T):\\n    if random.random() < accept_prob(S_old, S_new, T):\\n        print(True)\\n    else:\\n        print(False)\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            "
     },
     "metadata": {}
    }
   ],
   "source": [
    "import random\n",
    "from numpy import exp\n",
    "\n",
    "# from math import e\n",
    "\n",
    "\n",
    "def accept_prob(S_old, S_new, T):\n",
    "    # this is the acceptance \"probability\" in the greedy hill-climbing method\n",
    "    # where new solutions are accepted if and only if they are better\n",
    "    # than the old one.\n",
    "    # change it to be the acceptance probability in simulated annealing\n",
    "    return 1.0 if S_new > S_old else exp(-(S_old - S_new) / T)\n",
    "\n",
    "\n",
    "# the above function will be used as follows. this is shown just for\n",
    "# your information; you don't have to change anything here\n",
    "def accept(S_old, S_new, T):\n",
    "    if random.random() < accept_prob(S_old, S_new, T):\n",
    "        print(True)\n",
    "    else:\n",
    "        print(False)"
   ]
  },
  {
   "source": [
    "### Exercise 6: Simulated Annealing\n",
    "\n",
    "### --Intermediate\n",
    "\n",
    "1D simulated annealing: modify the program below to use simulated annealing instead of plain hill climbing. In simulated annealing the probability of accepting a solution that lowers the score is given by `prob = exp(-(S_old - S_new)/T)`. Setting the temperature T and gradually decreasing can be done in many ways, some of which lead to better outcomes than others. A good choice in this case is for example: `T = 2*max(0, ((steps-step*1.2)/steps))**3`.\n",
    "\n",
    "The code below uses the plain hill-climbing strategy to only go up towards a peak. As you can see, the hill-climbing strategy tends to get stuck in local optima."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Ended up at 4958, highest point is 883\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.Javascript object>",
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 36;\n                var nbb_unformatted_code = \"import math, random        \\t# just for generating random mountains                                 \\t \\nimport numpy as np\\n\\nn = 10000 # size of the problem: number of possible solutions x = 0, ..., n-1\\n\\n# generate random mountains                                                                               \\t \\ndef mountains(n):\\n    h = [0]*n\\n    for i in range(50):\\n        c = random.randint(20, n-20)\\n        w = random.randint(3, int(math.sqrt(n/5)))**2\\n        s = random.random()\\n        h[max(0, c-w):min(n, c+w)] = [h[i] + s*(w-abs(c-i)) for i in range(max(0, c-w), min(n, c+w))]\\n\\n    # scale the height so that the lowest point is 0.0 and the highest peak is 1.0\\n    low = min(h)\\n    high = max(h)\\n    h = [y - low for y in h]\\n    h = [y / (high-low) for y in h]\\n    return h\\n\\nh = mountains(n)\\n\\n# start at a random place\\nx0 = random.randint(1, n-1)\\nx = x0\\n\\n# keep climbing for 5000 steps\\nsteps = 5000\\n\\ndef main(h, x):\\n    n = len(h)\\n    # the climbing starts here\\n    for step in range(steps):\\n        # this is our temperature to to be used for simulated annealing\\n        # it starts large and decreases with each step. you don't have to change this\\n        T = 2*max(0, ((steps-step*1.2)/steps))**3\\n\\n        # let's try randomly moving (max. 1000 steps) left or right\\n        # making sure we don't fall off the edge of the world at 0 or n-1\\n        # the height at this point will be our candidate score, S_new\\n        # while the height at our current location will be S_old\\n        x_new = random.randint(max(0, x-1000), min(n-1, x+1000))\\n\\n        if h[x_new] > h[x]:\\n            x = x_new           # the new position is higher, go there\\n        else:\\n            if T != 0 and random.random() <= np.exp(-(h[x] - h[x_new])/T):\\n                x = x_new\\n            #if T == 0:\\n            #    pass\\n            #elif random.random() <= np.exp(-(h[x] - h[x_new])/T):\\n            #    x = x_new\\n    return x\\n\\nx = main(h, x0)\\nprint(\\\"Ended up at %d, highest point is %d\\\" % (x, np.argmax(h)))\";\n                var nbb_formatted_code = \"import math, random  # just for generating random mountains\\nimport numpy as np\\n\\nn = 10000  # size of the problem: number of possible solutions x = 0, ..., n-1\\n\\n# generate random mountains\\ndef mountains(n):\\n    h = [0] * n\\n    for i in range(50):\\n        c = random.randint(20, n - 20)\\n        w = random.randint(3, int(math.sqrt(n / 5))) ** 2\\n        s = random.random()\\n        h[max(0, c - w) : min(n, c + w)] = [\\n            h[i] + s * (w - abs(c - i)) for i in range(max(0, c - w), min(n, c + w))\\n        ]\\n\\n    # scale the height so that the lowest point is 0.0 and the highest peak is 1.0\\n    low = min(h)\\n    high = max(h)\\n    h = [y - low for y in h]\\n    h = [y / (high - low) for y in h]\\n    return h\\n\\n\\nh = mountains(n)\\n\\n# start at a random place\\nx0 = random.randint(1, n - 1)\\nx = x0\\n\\n# keep climbing for 5000 steps\\nsteps = 5000\\n\\n\\ndef main(h, x):\\n    n = len(h)\\n    # the climbing starts here\\n    for step in range(steps):\\n        # this is our temperature to to be used for simulated annealing\\n        # it starts large and decreases with each step. you don't have to change this\\n        T = 2 * max(0, ((steps - step * 1.2) / steps)) ** 3\\n\\n        # let's try randomly moving (max. 1000 steps) left or right\\n        # making sure we don't fall off the edge of the world at 0 or n-1\\n        # the height at this point will be our candidate score, S_new\\n        # while the height at our current location will be S_old\\n        x_new = random.randint(max(0, x - 1000), min(n - 1, x + 1000))\\n\\n        if h[x_new] > h[x]:\\n            x = x_new  # the new position is higher, go there\\n        else:\\n            if T != 0 and random.random() <= np.exp(-(h[x] - h[x_new]) / T):\\n                x = x_new\\n            # if T == 0:\\n            #    pass\\n            # elif random.random() <= np.exp(-(h[x] - h[x_new])/T):\\n            #    x = x_new\\n    return x\\n\\n\\nx = main(h, x0)\\nprint(\\\"Ended up at %d, highest point is %d\\\" % (x, np.argmax(h)))\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            "
     },
     "metadata": {}
    }
   ],
   "source": [
    "import math, random  # just for generating random mountains\n",
    "import numpy as np\n",
    "\n",
    "n = 10000  # size of the problem: number of possible solutions x = 0, ..., n-1\n",
    "\n",
    "# generate random mountains\n",
    "def mountains(n):\n",
    "    h = [0] * n\n",
    "    for i in range(50):\n",
    "        c = random.randint(20, n - 20)\n",
    "        w = random.randint(3, int(math.sqrt(n / 5))) ** 2\n",
    "        s = random.random()\n",
    "        h[max(0, c - w) : min(n, c + w)] = [\n",
    "            h[i] + s * (w - abs(c - i)) for i in range(max(0, c - w), min(n, c + w))\n",
    "        ]\n",
    "\n",
    "    # scale the height so that the lowest point is 0.0 and the highest peak is 1.0\n",
    "    low = min(h)\n",
    "    high = max(h)\n",
    "    h = [y - low for y in h]\n",
    "    h = [y / (high - low) for y in h]\n",
    "    return h\n",
    "\n",
    "\n",
    "h = mountains(n)\n",
    "\n",
    "# start at a random place\n",
    "x0 = random.randint(1, n - 1)\n",
    "x = x0\n",
    "\n",
    "# keep climbing for 5000 steps\n",
    "steps = 5000\n",
    "\n",
    "\n",
    "def main(h, x):\n",
    "    n = len(h)\n",
    "    # the climbing starts here\n",
    "    for step in range(steps):\n",
    "        # this is our temperature to to be used for simulated annealing\n",
    "        # it starts large and decreases with each step. you don't have to change this\n",
    "        T = 2 * max(0, ((steps - step * 1.2) / steps)) ** 3\n",
    "\n",
    "        # let's try randomly moving (max. 1000 steps) left or right\n",
    "        # making sure we don't fall off the edge of the world at 0 or n-1\n",
    "        # the height at this point will be our candidate score, S_new\n",
    "        # while the height at our current location will be S_old\n",
    "        x_new = random.randint(max(0, x - 1000), min(n - 1, x + 1000))\n",
    "\n",
    "        if h[x_new] > h[x]:\n",
    "            x = x_new  # the new position is higher, go there\n",
    "        else:\n",
    "            if T != 0 and random.random() <= np.exp(-(h[x] - h[x_new]) / T):\n",
    "                x = x_new\n",
    "            # if T == 0:\n",
    "            #    pass\n",
    "            # elif random.random() <= np.exp(-(h[x] - h[x_new])/T):\n",
    "            #    x = x_new\n",
    "    return x\n",
    "\n",
    "\n",
    "x = main(h, x0)\n",
    "print(\"Ended up at %d, highest point is %d\" % (x, np.argmax(h)))"
   ]
  },
  {
   "source": [
    "### --Advanced\n",
    "\n",
    "Let's use simulated annealing to solve a simple two-dimensional optimization problem. The following code runs 50 optimization tracks in parallel (at the same time). It currently only looks around the current solution and only accepts moves that go up. Modify the program so that it uses simulated annealing.\n",
    "\n",
    "Remember that the probability of accepting a solution that lowers the score is given by `prob = exp(–(S_old - S_new)/T)`. Remember to also adjust the temperature in a way that it decreases as the simulation goes on, and to handle T=0 case correctly.\n",
    "\n",
    "Your goal is to ensure that on the average, at least 30 of the optimization tracks find the global optimum (the highest peak)."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "39\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.Javascript object>",
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 37;\n                var nbb_unformatted_code = \"import numpy as np\\nimport random\\n\\nN = 100     # size of the problem is N x N                                      \\nsteps = 3000    # total number of iterations                                        \\ntracks = 50\\n\\n# generate a landscape with multiple local optima                                          \\ndef generator(x, y, x0=0.0, y0=0.0):\\n    return np.sin((x/N-x0)*np.pi)+np.sin((y/N-y0)*np.pi)+\\\\\\n        .07*np.cos(12*(x/N-x0)*np.pi)+.07*np.cos(12*(y/N-y0)*np.pi)\\n\\nx0 = np.random.random() - 0.5\\ny0 = np.random.random() - 0.5\\nh = np.fromfunction(np.vectorize(generator), (N, N), x0=x0, y0=y0, dtype=int)\\npeak_x, peak_y = np.unravel_index(np.argmax(h), h.shape)\\n\\n# starting points                                                               \\nx = np.random.randint(0, N, tracks)\\ny = np.random.randint(0, N, tracks)\\n\\ndef main():\\n    global x\\n    global y\\n\\n    for step in range(steps):\\n        # add a temperature schedule here\\n        T = 2*max(0, ((steps-step*1.2)/steps))**3\\n        # update solutions on each search track                                     \\n        for i in range(tracks):\\n            # try a new solution near the current one                               \\n            x_new = np.random.randint(max(0, x[i]-2), min(N, x[i]+2+1))\\n            y_new = np.random.randint(max(0, y[i]-2), min(N, y[i]+2+1))\\n            S_old = h[x[i], y[i]]\\n            S_new = h[x_new, y_new]\\n\\n            # change this to use simulated annealing\\n            if S_new > S_old:\\n                x[i], y[i] = x_new, y_new   # new solution is better, go there       \\n            else:\\n                if T != 0 and random.random() <= np.exp(-(S_old - S_new)/T):\\n                    x[i], y[i] = x_new, y_new\\n\\n    # Number of tracks found the peak\\n    print(sum([x[j] == peak_x and y[j] == peak_y for j in range(tracks)])) \\n    \\nmain()\";\n                var nbb_formatted_code = \"import numpy as np\\nimport random\\n\\nN = 100  # size of the problem is N x N\\nsteps = 3000  # total number of iterations\\ntracks = 50\\n\\n# generate a landscape with multiple local optima\\ndef generator(x, y, x0=0.0, y0=0.0):\\n    return (\\n        np.sin((x / N - x0) * np.pi)\\n        + np.sin((y / N - y0) * np.pi)\\n        + 0.07 * np.cos(12 * (x / N - x0) * np.pi)\\n        + 0.07 * np.cos(12 * (y / N - y0) * np.pi)\\n    )\\n\\n\\nx0 = np.random.random() - 0.5\\ny0 = np.random.random() - 0.5\\nh = np.fromfunction(np.vectorize(generator), (N, N), x0=x0, y0=y0, dtype=int)\\npeak_x, peak_y = np.unravel_index(np.argmax(h), h.shape)\\n\\n# starting points\\nx = np.random.randint(0, N, tracks)\\ny = np.random.randint(0, N, tracks)\\n\\n\\ndef main():\\n    global x\\n    global y\\n\\n    for step in range(steps):\\n        # add a temperature schedule here\\n        T = 2 * max(0, ((steps - step * 1.2) / steps)) ** 3\\n        # update solutions on each search track\\n        for i in range(tracks):\\n            # try a new solution near the current one\\n            x_new = np.random.randint(max(0, x[i] - 2), min(N, x[i] + 2 + 1))\\n            y_new = np.random.randint(max(0, y[i] - 2), min(N, y[i] + 2 + 1))\\n            S_old = h[x[i], y[i]]\\n            S_new = h[x_new, y_new]\\n\\n            # change this to use simulated annealing\\n            if S_new > S_old:\\n                x[i], y[i] = x_new, y_new  # new solution is better, go there\\n            else:\\n                if T != 0 and random.random() <= np.exp(-(S_old - S_new) / T):\\n                    x[i], y[i] = x_new, y_new\\n\\n    # Number of tracks found the peak\\n    print(sum([x[j] == peak_x and y[j] == peak_y for j in range(tracks)]))\\n\\n\\nmain()\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            "
     },
     "metadata": {}
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "N = 100  # size of the problem is N x N\n",
    "steps = 3000  # total number of iterations\n",
    "tracks = 50\n",
    "\n",
    "# generate a landscape with multiple local optima\n",
    "def generator(x, y, x0=0.0, y0=0.0):\n",
    "    return (\n",
    "        np.sin((x / N - x0) * np.pi)\n",
    "        + np.sin((y / N - y0) * np.pi)\n",
    "        + 0.07 * np.cos(12 * (x / N - x0) * np.pi)\n",
    "        + 0.07 * np.cos(12 * (y / N - y0) * np.pi)\n",
    "    )\n",
    "\n",
    "\n",
    "x0 = np.random.random() - 0.5\n",
    "y0 = np.random.random() - 0.5\n",
    "h = np.fromfunction(np.vectorize(generator), (N, N), x0=x0, y0=y0, dtype=int)\n",
    "peak_x, peak_y = np.unravel_index(np.argmax(h), h.shape)\n",
    "\n",
    "# starting points\n",
    "x = np.random.randint(0, N, tracks)\n",
    "y = np.random.randint(0, N, tracks)\n",
    "\n",
    "\n",
    "def main():\n",
    "    global x\n",
    "    global y\n",
    "\n",
    "    for step in range(steps):\n",
    "        # add a temperature schedule here\n",
    "        T = 2 * max(0, ((steps - step * 1.2) / steps)) ** 3\n",
    "        # update solutions on each search track\n",
    "        for i in range(tracks):\n",
    "            # try a new solution near the current one\n",
    "            x_new = np.random.randint(max(0, x[i] - 2), min(N, x[i] + 2 + 1))\n",
    "            y_new = np.random.randint(max(0, y[i] - 2), min(N, y[i] + 2 + 1))\n",
    "            S_old = h[x[i], y[i]]\n",
    "            S_new = h[x_new, y_new]\n",
    "\n",
    "            # change this to use simulated annealing\n",
    "            if S_new > S_old:\n",
    "                x[i], y[i] = x_new, y_new  # new solution is better, go there\n",
    "            else:\n",
    "                if T != 0 and random.random() <= np.exp(-(S_old - S_new) / T):\n",
    "                    x[i], y[i] = x_new, y_new\n",
    "\n",
    "    # Number of tracks found the peak\n",
    "    print(sum([x[j] == peak_x and y[j] == peak_y for j in range(tracks)]))\n",
    "\n",
    "\n",
    "main()"
   ]
  },
  {
   "source": [
    "# Dealing with uncertainty"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## I.Probability fundamentals"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Exercise 7: Flip the coin\n",
    "\n",
    "### -- Advanced\n",
    "\n",
    "Write a program that generates 10000 random zeros and ones where the probability of one is p1 and the probability of zero is 1-p1 (hint: `np.random.choice([0,1], p=[1-p1, p1], size=10000)`), counts the number of occurrences of 5 consecutive ones (\"11111\") in the sequence, and outputs this number as a return value. Check that for p1 = 2/3, the count is close to 10000 x (2/3)^5 ≈ 1316.9."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1352\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.Javascript object>",
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 38;\n                var nbb_unformatted_code = \"import numpy as np\\n\\ndef generate(p1):\\n    # change this so that it generates 10000 random zeros and ones\\n    # where the probability of one is p1\\n    seq = np.empty(10000)\\n    seq = np.random.choice([0,1], p=[1-p1, p1], size=10000)\\n    return seq\\n\\ndef count(seq):\\n    # insert code to return the number of occurrences of 11111 in the sequence\\n    seq = ''.join(map(str, seq))\\n    tofind = '11111'\\n    found_num = 0\\n    i = seq.find(tofind)\\n    while i != -1:\\n        found_num += 1\\n        i = seq.find(tofind, i+1)\\n    return found_num\\n\\ndef main(p1):\\n    seq = generate(p1)\\n    return count(seq)\\n\\nprint(main(2/3))\";\n                var nbb_formatted_code = \"import numpy as np\\n\\n\\ndef generate(p1):\\n    # change this so that it generates 10000 random zeros and ones\\n    # where the probability of one is p1\\n    seq = np.empty(10000)\\n    seq = np.random.choice([0, 1], p=[1 - p1, p1], size=10000)\\n    return seq\\n\\n\\ndef count(seq):\\n    # insert code to return the number of occurrences of 11111 in the sequence\\n    seq = \\\"\\\".join(map(str, seq))\\n    tofind = \\\"11111\\\"\\n    found_num = 0\\n    i = seq.find(tofind)\\n    while i != -1:\\n        found_num += 1\\n        i = seq.find(tofind, i + 1)\\n    return found_num\\n\\n\\ndef main(p1):\\n    seq = generate(p1)\\n    return count(seq)\\n\\n\\nprint(main(2 / 3))\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            "
     },
     "metadata": {}
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def generate(p1):\n",
    "    # change this so that it generates 10000 random zeros and ones\n",
    "    # where the probability of one is p1\n",
    "    seq = np.empty(10000)\n",
    "    seq = np.random.choice([0, 1], p=[1 - p1, p1], size=10000)\n",
    "    return seq\n",
    "\n",
    "\n",
    "def count(seq):\n",
    "    # insert code to return the number of occurrences of 11111 in the sequence\n",
    "    seq = \"\".join(map(str, seq))\n",
    "    tofind = \"11111\"\n",
    "    found_num = 0\n",
    "    i = seq.find(tofind)\n",
    "    while i != -1:\n",
    "        found_num += 1\n",
    "        i = seq.find(tofind, i + 1)\n",
    "    return found_num\n",
    "\n",
    "\n",
    "def main(p1):\n",
    "    seq = generate(p1)\n",
    "    return count(seq)\n",
    "\n",
    "\n",
    "print(main(2 / 3))"
   ]
  },
  {
   "source": [
    "*The probability of \"11111\" at any given position in the sequence can be calculated as (2/3)^5 ≈ 0.13169. The number of occurrences is close to 10000 times this: 1316.9. To be more precise, the expected number of occurrences is about 0.13169 x 9996 ≈ 1316.3, because there are only 9996 places for a subsequence of length five in a sequence of 10000. The actual number will usually (in fact, with over 99% probability) be somewhere between 1230 and 1404. We check the solution allowing for an even wider margin that covers 99.99% of the cases.*"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Exercise 8: Fishing in the Nordics\n",
    "\n",
    "### -- Advanced\n",
    "\n",
    "Suppose we also happen to know the gender of the lottery winner. Here are same OECD statistics as above broken down by gender:\n",
    "\n",
    "|Country\t|Population\t|Male fishers\t|Female fishers\t|Fishers (total)|\n",
    "|:----------|:----------|:--------------|:--------------|:--------------|\n",
    "|Denmark\t|5,615,000\t|1822\t        |69\t            |1891           |\n",
    "|Finland\t|5,439,000\t|2575\t        |77\t            |2652           |\n",
    "|Iceland\t|324,000\t|3400\t        |400            |3800           |\n",
    "|Norway\t    |5,080,000\t|11,291\t        |320\t        |11,611         |\n",
    "|Sweden \t|9,609,000\t|1731\t        |26\t            |1757           |\n",
    "|TOTAL\t    |26,067,000\t|20,819\t        |892\t        |21,711         |\n",
    "\n",
    "Write a function that uses the above numbers and tries to guess the nationality of the winner when we know that the winner is a fisher and their gender (either female or male).\n",
    "\n",
    "The argument of the function should be the gender of the winner ('female' or 'male'). The return value of the function should be a pair (country, probability) where country is the most likely nationality of the winner and probability is the probability of the country being the nationality of the winner."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "if the winner is male, my guess is he's from Norway; probability 54.23%\nif the winner is female, my guess is she's from Iceland; probability 44.84%\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.Javascript object>",
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 39;\n                var nbb_unformatted_code = \"countries = ['Denmark', 'Finland', 'Iceland', 'Norway', 'Sweden']\\npopulations = [5615000, 5439000, 324000, 5080000, 9609000]\\nmale_fishers = [1822, 2575, 3400, 11291, 1731]\\nfemale_fishers = [69, 77, 400, 320, 26] \\n\\ndef guess(winner_gender):\\n    \\\"\\\"\\\" guess the nationality of the winner\\n        when we know that the winner is a fisher and their gender\\n    \\\"\\\"\\\"\\n    # P(nat\\u00a0\\u2223\\u00a0male_fisher)= male_fishers(nat)\\u00f7fishers(total)\\n    # P(nat\\u00a0\\u2223\\u00a0female_fisher)= female_fishers(nat)\\u00f7fishers(total)\\n\\n    if winner_gender == 'female':\\n        fishers = female_fishers\\n    else:\\n        fishers = male_fishers\\n\\n    # write your solution here\\n    guess = None\\n    biggest = 0.0\\n    for country, fisher in zip(countries, fishers):\\n        frac = fisher / sum(fishers) * 100\\n        if frac > biggest:\\n            guess = country\\n            biggest = frac\\n    return (guess, biggest)  \\n\\ndef main():\\n    country, fraction = guess(\\\"male\\\")\\n    print(\\\"if the winner is male, my guess is he's from %s; probability %.2f%%\\\" % (country, fraction))\\n    country, fraction = guess(\\\"female\\\")\\n    print(\\\"if the winner is female, my guess is she's from %s; probability %.2f%%\\\" % (country, fraction))\\n\\nmain()\";\n                var nbb_formatted_code = \"countries = [\\\"Denmark\\\", \\\"Finland\\\", \\\"Iceland\\\", \\\"Norway\\\", \\\"Sweden\\\"]\\npopulations = [5615000, 5439000, 324000, 5080000, 9609000]\\nmale_fishers = [1822, 2575, 3400, 11291, 1731]\\nfemale_fishers = [69, 77, 400, 320, 26]\\n\\n\\ndef guess(winner_gender):\\n    \\\"\\\"\\\"guess the nationality of the winner\\n    when we know that the winner is a fisher and their gender\\n    \\\"\\\"\\\"\\n    # P(nat\\u00a0\\u2223\\u00a0male_fisher)= male_fishers(nat)\\u00f7fishers(total)\\n    # P(nat\\u00a0\\u2223\\u00a0female_fisher)= female_fishers(nat)\\u00f7fishers(total)\\n\\n    if winner_gender == \\\"female\\\":\\n        fishers = female_fishers\\n    else:\\n        fishers = male_fishers\\n\\n    # write your solution here\\n    guess = None\\n    biggest = 0.0\\n    for country, fisher in zip(countries, fishers):\\n        frac = fisher / sum(fishers) * 100\\n        if frac > biggest:\\n            guess = country\\n            biggest = frac\\n    return (guess, biggest)\\n\\n\\ndef main():\\n    country, fraction = guess(\\\"male\\\")\\n    print(\\n        \\\"if the winner is male, my guess is he's from %s; probability %.2f%%\\\"\\n        % (country, fraction)\\n    )\\n    country, fraction = guess(\\\"female\\\")\\n    print(\\n        \\\"if the winner is female, my guess is she's from %s; probability %.2f%%\\\"\\n        % (country, fraction)\\n    )\\n\\n\\nmain()\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            "
     },
     "metadata": {}
    }
   ],
   "source": [
    "countries = [\"Denmark\", \"Finland\", \"Iceland\", \"Norway\", \"Sweden\"]\n",
    "populations = [5615000, 5439000, 324000, 5080000, 9609000]\n",
    "male_fishers = [1822, 2575, 3400, 11291, 1731]\n",
    "female_fishers = [69, 77, 400, 320, 26]\n",
    "\n",
    "\n",
    "def guess(winner_gender):\n",
    "    \"\"\"guess the nationality of the winner\n",
    "    when we know that the winner is a fisher and their gender\n",
    "    \"\"\"\n",
    "    # P(nat ∣ male_fisher)= male_fishers(nat)÷fishers(total)\n",
    "    # P(nat ∣ female_fisher)= female_fishers(nat)÷fishers(total)\n",
    "\n",
    "    if winner_gender == \"female\":\n",
    "        fishers = female_fishers\n",
    "    else:\n",
    "        fishers = male_fishers\n",
    "\n",
    "    # write your solution here\n",
    "    guess = None\n",
    "    biggest = 0.0\n",
    "    for country, fisher in zip(countries, fishers):\n",
    "        frac = fisher / sum(fishers) * 100\n",
    "        if frac > biggest:\n",
    "            guess = country\n",
    "            biggest = frac\n",
    "    return (guess, biggest)\n",
    "\n",
    "\n",
    "def main():\n",
    "    country, fraction = guess(\"male\")\n",
    "    print(\n",
    "        \"if the winner is male, my guess is he's from %s; probability %.2f%%\"\n",
    "        % (country, fraction)\n",
    "    )\n",
    "    country, fraction = guess(\"female\")\n",
    "    print(\n",
    "        \"if the winner is female, my guess is she's from %s; probability %.2f%%\"\n",
    "        % (country, fraction)\n",
    "    )\n",
    "\n",
    "\n",
    "main()"
   ]
  },
  {
   "source": [
    "## II.The Bayes Rule"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Exercise 9: Block or not\n",
    "\n",
    "### -- Advanced\n",
    "\n",
    "Let's suppose you have a social media account on Instagram, Twitter, or some other platform. (Just in case you don't, it doesn't matter. We'll fill you in with the relevant information.) You check your account and notice that you have a new follower – this means that another user has decided to start following you to see things that you post. You don't recognize the person, and their username (or \"handle\" as it's called) is a little strange: John37330190. You don't want to have creepy bots following you, so you wonder. To decide whether you should block the new follower, you decide to use the Bayes rule!\n",
    "\n",
    "Suppose we know the probability that a new follower is a bot. You'll be writing a program that takes this value as an input. For now, let's just call this value P(bot). You'll also be given the probability that the username of a bot account includes an 8-digit number, which we'll call P(8-digits | bot), as well as the same probability for human (non-bot) accounts, P(8-digits | human).\n",
    "\n",
    "To use the Bayes rule, we'll also need to know the probability that a new follower (can be either bot or human) has an 8-digit number in their username, P(8-digits). The nice thing is that we can calculate P(8-digits) from the above information. The formula is as follows:\n",
    "\n",
    "`P(8-digits) = P(8-digits | bot) x P(bot) + P(8-digits | human) x P(human)`\n",
    "\n",
    "Remember that you can get P(human) simply as 1–P(bot), since these are the only options. (We consider business and other accounts as \"human\" as long as they aren't bots.)\n",
    "\n",
    "Write a program that takes as input the probability of a follower being a bot (pbot), the probability of a bot having a username with 8 digits (p8_bot), and the probability of a human having a username with 8 digits (p8_human). The values for these inputs are free for you to choose, but they have to be probabilitites, so they have to be between 0 and 1.\n",
    "\n",
    "Using the numbers you give the program calculate P(8-digits) and then use it and the Bayes rule to calculate and print out the probability of the new follower being a bot, P(bot | 8-digits):\n",
    "\n",
    "`P(bot | 8-digits) =  P(8-digits | bot) x P(bot) / P(8-digits)`."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.64\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.Javascript object>",
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 40;\n                var nbb_unformatted_code = \"def bot8(pbot, p8_bot, p8_human):\\n    # P(8-digits) = P(8-digits | bot) x P(bot) + P(8-digits | human) x P(human)\\n    p8 = p8_bot * pbot + p8_human * (1 - pbot)\\n    # P(bot | 8-digits) =  P(8-digits | bot) x P(bot) / P(8-digits)\\n    pbot_8 = p8_bot * pbot / p8\\n    print(pbot_8)\\n\\n# you can change these values to test your program with different values\\npbot = 0.1\\np8_bot = 0.8\\np8_human = 0.05\\n\\nbot8(pbot, p8_bot, p8_human)\";\n                var nbb_formatted_code = \"def bot8(pbot, p8_bot, p8_human):\\n    # P(8-digits) = P(8-digits | bot) x P(bot) + P(8-digits | human) x P(human)\\n    p8 = p8_bot * pbot + p8_human * (1 - pbot)\\n    # P(bot | 8-digits) =  P(8-digits | bot) x P(bot) / P(8-digits)\\n    pbot_8 = p8_bot * pbot / p8\\n    print(pbot_8)\\n\\n\\n# you can change these values to test your program with different values\\npbot = 0.1\\np8_bot = 0.8\\np8_human = 0.05\\n\\nbot8(pbot, p8_bot, p8_human)\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            "
     },
     "metadata": {}
    }
   ],
   "source": [
    "def bot8(pbot, p8_bot, p8_human):\n",
    "    # P(8-digits) = P(8-digits | bot) x P(bot) + P(8-digits | human) x P(human)\n",
    "    p8 = p8_bot * pbot + p8_human * (1 - pbot)\n",
    "    # P(bot | 8-digits) =  P(8-digits | bot) x P(bot) / P(8-digits)\n",
    "    pbot_8 = p8_bot * pbot / p8\n",
    "    print(pbot_8)\n",
    "\n",
    "\n",
    "# you can change these values to test your program with different values\n",
    "pbot = 0.1\n",
    "p8_bot = 0.8\n",
    "p8_human = 0.05\n",
    "\n",
    "bot8(pbot, p8_bot, p8_human)"
   ]
  },
  {
   "source": [
    "## III.Naive Bayes classifier\n",
    "\n",
    "### Exercise 10: Naive Bayes classifier\n",
    "\n",
    "### -- Advanced\n",
    "\n",
    "We have two dice in our desk drawer. One is a normal, plain die with six sides such that each of the sides comes up with equal 1/6 probability. The other one is a loaded die that also has six sides, but that however gives the outcome 6 with every second try on the average, the other five sides being equally probable.\n",
    "\n",
    "Thus with the first, normal die the probabilities of each side are the same, 0.167 (or 16.7 %). With the second, loaded die, the probability of 6 is 0.5 (or 50 %) and each of the other five sides has probability 0.1 (or 10 %).\n",
    "\n",
    "The following program gets as its input the choice of the die and then simulates a sequence of ten rolls.\n",
    "\n",
    "Your task: starting from the odds 1:1, use the naive Bayes method to update the odds after each outcome to decide which of the dice is more likely. Edit the function bayes so that it returns True if the most likely die is the loaded one, and False otherwise. Remember to be careful with the indices when accessing list elements!"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Rolling a normal die\nrolled 6\nrolled 4\nrolled 1\nrolled 4\nrolled 2\nrolled 6\nrolled 6\nrolled 6\nrolled 6\nrolled 4\nI think loaded\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.Javascript object>",
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 41;\n                var nbb_unformatted_code = \"import numpy as np\\n\\np1 = [1/6, 1/6, 1/6, 1/6, 1/6, 1/6]   # normal\\np2 = [0.1, 0.1, 0.1, 0.1, 0.1, 0.5]   # loaded\\n\\ndef roll(loaded):\\n    if loaded:\\n        print(\\\"Rolling a loaded die\\\")\\n        p = p2\\n    else:\\n        print(\\\"Rolling a normal die\\\")\\n        p = p1\\n\\n    # roll the dice 10 times\\n    # add 1 to get dice rolls from 1 to 6 instead of 0 to 5\\n    sequence = np.random.choice(6, size=10, p=p) + 1 \\n    for roll in sequence:\\n        print(\\\"rolled %d\\\" % roll)\\n    return sequence\\n\\ndef bayes(sequence):\\n    \\\"\\\"\\\"\\n        Starting from the odds 1:1, use the naive Bayes method to update the odds after each outcome to decide which of the dice is more likely\\n        Edit the function bayes so that it returns True if the most likely die is the loaded one, and False otherwise.\\n    \\\"\\\"\\\"\\n    odds = 1.0           # start with odds 1:1\\n    for roll in sequence:\\n        odds *= p2[roll-1] / p1[roll-1]\\n        # edit here to update the odds\\n    return True if odds > 1 else False\\n\\nsequence = roll(False)  # False = normal die, try changing to True\\nif bayes(sequence):\\n    print(\\\"I think loaded\\\")\\nelse:\\n    print(\\\"I think normal\\\")\";\n                var nbb_formatted_code = \"import numpy as np\\n\\np1 = [1 / 6, 1 / 6, 1 / 6, 1 / 6, 1 / 6, 1 / 6]  # normal\\np2 = [0.1, 0.1, 0.1, 0.1, 0.1, 0.5]  # loaded\\n\\n\\ndef roll(loaded):\\n    if loaded:\\n        print(\\\"Rolling a loaded die\\\")\\n        p = p2\\n    else:\\n        print(\\\"Rolling a normal die\\\")\\n        p = p1\\n\\n    # roll the dice 10 times\\n    # add 1 to get dice rolls from 1 to 6 instead of 0 to 5\\n    sequence = np.random.choice(6, size=10, p=p) + 1\\n    for roll in sequence:\\n        print(\\\"rolled %d\\\" % roll)\\n    return sequence\\n\\n\\ndef bayes(sequence):\\n    \\\"\\\"\\\"\\n    Starting from the odds 1:1, use the naive Bayes method to update the odds after each outcome to decide which of the dice is more likely\\n    Edit the function bayes so that it returns True if the most likely die is the loaded one, and False otherwise.\\n    \\\"\\\"\\\"\\n    odds = 1.0  # start with odds 1:1\\n    for roll in sequence:\\n        odds *= p2[roll - 1] / p1[roll - 1]\\n        # edit here to update the odds\\n    return True if odds > 1 else False\\n\\n\\nsequence = roll(False)  # False = normal die, try changing to True\\nif bayes(sequence):\\n    print(\\\"I think loaded\\\")\\nelse:\\n    print(\\\"I think normal\\\")\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            "
     },
     "metadata": {}
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "p1 = [1 / 6, 1 / 6, 1 / 6, 1 / 6, 1 / 6, 1 / 6]  # normal\n",
    "p2 = [0.1, 0.1, 0.1, 0.1, 0.1, 0.5]  # loaded\n",
    "\n",
    "\n",
    "def roll(loaded):\n",
    "    if loaded:\n",
    "        print(\"Rolling a loaded die\")\n",
    "        p = p2\n",
    "    else:\n",
    "        print(\"Rolling a normal die\")\n",
    "        p = p1\n",
    "\n",
    "    # roll the dice 10 times\n",
    "    # add 1 to get dice rolls from 1 to 6 instead of 0 to 5\n",
    "    sequence = np.random.choice(6, size=10, p=p) + 1\n",
    "    for roll in sequence:\n",
    "        print(\"rolled %d\" % roll)\n",
    "    return sequence\n",
    "\n",
    "\n",
    "def bayes(sequence):\n",
    "    \"\"\"\n",
    "    Starting from the odds 1:1, use the naive Bayes method to update the odds after each outcome to decide which of the dice is more likely\n",
    "    Edit the function bayes so that it returns True if the most likely die is the loaded one, and False otherwise.\n",
    "    \"\"\"\n",
    "    odds = 1.0  # start with odds 1:1\n",
    "    for roll in sequence:\n",
    "        odds *= p2[roll - 1] / p1[roll - 1]\n",
    "        # edit here to update the odds\n",
    "    return True if odds > 1 else False\n",
    "\n",
    "\n",
    "sequence = roll(False)  # False = normal die, try changing to True\n",
    "if bayes(sequence):\n",
    "    print(\"I think loaded\")\n",
    "else:\n",
    "    print(\"I think normal\")"
   ]
  },
  {
   "source": [
    "# Machine learning"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## I.Linear regression"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Exercise 11: Real estate price predictions\n",
    "\n",
    "### -- Advanced\n",
    "\n",
    "Edit the following program so that it can process multiple cabins that may be described by any number of details (like five below), at the same time. You can assume that each of the lists contained in the list x and the coefficients c contain the same number of elements."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "258250\n76100\n492750\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.Javascript object>",
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 42;\n                var nbb_unformatted_code = \"# input values for three m\\u00f6kkis: size, size of sauna, distance to water, number of indoor bathrooms, \\n# proximity of neighbors\\nX = [[66, 5, 15, 2, 500], \\n     [21, 3, 50, 1, 100], \\n     [120, 15, 5, 2, 1200]]\\nc = [3000, 200, -50, 5000, 100]    # coefficient values\\n\\ndef predict(X, c):\\n    for cabin in range(len(X)):\\n        price = sum(map(lambda xx,cc: xx*cc, X[cabin], c))\\n        print(price)\\n\\npredict(X, c)\";\n                var nbb_formatted_code = \"# input values for three m\\u00f6kkis: size, size of sauna, distance to water, number of indoor bathrooms,\\n# proximity of neighbors\\nX = [[66, 5, 15, 2, 500], [21, 3, 50, 1, 100], [120, 15, 5, 2, 1200]]\\nc = [3000, 200, -50, 5000, 100]  # coefficient values\\n\\n\\ndef predict(X, c):\\n    for cabin in range(len(X)):\\n        price = sum(map(lambda xx, cc: xx * cc, X[cabin], c))\\n        print(price)\\n\\n\\npredict(X, c)\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            "
     },
     "metadata": {}
    }
   ],
   "source": [
    "# input values for three mökkis: size, size of sauna, distance to water, number of indoor bathrooms,\n",
    "# proximity of neighbors\n",
    "X = [[66, 5, 15, 2, 500], [21, 3, 50, 1, 100], [120, 15, 5, 2, 1200]]\n",
    "c = [3000, 200, -50, 5000, 100]  # coefficient values\n",
    "\n",
    "\n",
    "def predict(X, c):\n",
    "    for cabin in range(len(X)):\n",
    "        price = sum(map(lambda xx, cc: xx * cc, X[cabin], c))\n",
    "        print(price)\n",
    "\n",
    "\n",
    "predict(X, c)"
   ]
  },
  {
   "source": [
    "### Exercise 12: Least squares\n",
    "\n",
    "### --Advanced\n",
    "\n",
    "Write a program that calculates the squared error for multiple sets of coefficient values and prints out the index of the set that yields the smallest squared error: this is a poor man's version of the least squares method where we only consider a fixed set of alternative coefficient vectors instead of finding the global optimum."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "the best set is set 1\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.Javascript object>",
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 43;\n                var nbb_unformatted_code = \"import numpy as np\\n\\n# data\\nX = np.array([[66, 5, 15, 2, 500], \\n              [21, 3, 50, 1, 100], \\n              [120, 15, 5, 2, 1200]])\\ny = np.array([250000, 60000, 525000])\\n\\n# alternative sets of coefficient values\\nc = np.array([[3000, 200 , -50, 5000, 100], \\n              [2000, -250, -100, 150, 250], \\n              [3000, -100, -150, 0, 150]])   \\n\\ndef find_best(X, y, c):\\n    smallest_error = np.Inf\\n    best_index = -1\\n    for ind, coeff in enumerate(c):\\n        sqerr = sum((y - X@coeff)**2)\\n        if sqerr < smallest_error:\\n            best_index = ind\\n            smallest_error = sqerr\\n    print(\\\"the best set is set %d\\\" % best_index)\\n\\nfind_best(X, y, c)\";\n                var nbb_formatted_code = \"import numpy as np\\n\\n# data\\nX = np.array([[66, 5, 15, 2, 500], [21, 3, 50, 1, 100], [120, 15, 5, 2, 1200]])\\ny = np.array([250000, 60000, 525000])\\n\\n# alternative sets of coefficient values\\nc = np.array(\\n    [\\n        [3000, 200, -50, 5000, 100],\\n        [2000, -250, -100, 150, 250],\\n        [3000, -100, -150, 0, 150],\\n    ]\\n)\\n\\n\\ndef find_best(X, y, c):\\n    smallest_error = np.Inf\\n    best_index = -1\\n    for ind, coeff in enumerate(c):\\n        sqerr = sum((y - X @ coeff) ** 2)\\n        if sqerr < smallest_error:\\n            best_index = ind\\n            smallest_error = sqerr\\n    print(\\\"the best set is set %d\\\" % best_index)\\n\\n\\nfind_best(X, y, c)\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            "
     },
     "metadata": {}
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# data\n",
    "X = np.array([[66, 5, 15, 2, 500], [21, 3, 50, 1, 100], [120, 15, 5, 2, 1200]])\n",
    "y = np.array([250000, 60000, 525000])\n",
    "\n",
    "# alternative sets of coefficient values\n",
    "c = np.array(\n",
    "    [\n",
    "        [3000, 200, -50, 5000, 100],\n",
    "        [2000, -250, -100, 150, 250],\n",
    "        [3000, -100, -150, 0, 150],\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "def find_best(X, y, c):\n",
    "    smallest_error = np.Inf\n",
    "    best_index = -1\n",
    "    for ind, coeff in enumerate(c):\n",
    "        sqerr = sum((y - X @ coeff) ** 2)\n",
    "        if sqerr < smallest_error:\n",
    "            best_index = ind\n",
    "            smallest_error = sqerr\n",
    "    print(\"the best set is set %d\" % best_index)\n",
    "\n",
    "\n",
    "find_best(X, y, c)"
   ]
  },
  {
   "source": [
    "### Exercise 13: Predictions with more data\n",
    "\n",
    "### -- Advanced\n",
    "\n",
    "Write a program that reads cabin details and prices from a CSV file (a standard format for tabular data) and fits a linear regression model to it. The program should be able to handle any number of data points (cabins) described by any number of features (like size, size of sauna, number of bathrooms, ...).\n",
    "\n",
    "You can read a CSV file with the function `np.genfromtxt(datafile, skip_header=1)`. This will return a numpy array that contains the feature data in the columns preceding the last one, and the price data in the last column. The option skip_header=1 just means that the first line in the file is supposed to contain just the column names and shouldn't be included in the actual data.\n",
    "\n",
    "The output of the program should be the **estimated** coefficients and the **predicted or \"fitted\"** prices for the same set of cabins used to estimate the parameters. So if you fit the model using data for six cabins with known prices, the program will print out the prices that the model predicts for those six cabins (even if the actual prices are already given in the data).\n",
    "\n",
    "Note that here we will not actually only simulate the file input using Python's **io.StringIO** function that takes an input string and pretends that the contents is coming from a file. In practice, you would just name the input file that contains the data in the same format as the string input below."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[2989.6  800.6  -44.8 3890.8   99.8]\n",
      "[127907.6 222269.8 143604.5 268017.6 460686.6 406959.9]\n",
      "<ipython-input-44-e41c6fd3422e>:24: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  c = np.linalg.lstsq(x, y)[0]\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.Javascript object>",
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 44;\n                var nbb_unformatted_code = \"import numpy as np\\nfrom io import StringIO\\n\\ninput_string = '''\\n25 2 50 1 500 127900\\n39 3 10 1 1000 222100\\n13 2 13 1 1000 143750\\n82 5 20 2 120 268000\\n130 6 10 2 600 460700\\n115 6 10 1 550 407000\\n'''\\n\\nnp.set_printoptions(precision=1)    # this just changes the output settings for easier reading\\n \\ndef fit_model(input_file):\\n    # read the data in and fit it. the values below are placeholder values\\n    c = np.asarray([])  # coefficients of the linear regression\\n    x = np.asarray([])  # input data to the linear regression\\n    # This will return a numpy array that contains the feature data in the columns preceding the last one,\\n    # and the price data in the last column.\\n    data = np.genfromtxt(input_file, skip_header=1)\\n    x = data[:, :-1]\\n    y = data[:, -1]\\n    c = np.linalg.lstsq(x, y)[0]\\n    print(c)\\n    print(x @ c)\\n\\n# simulate reading a file\\ninput_file = StringIO(input_string)\\nfit_model(input_file)\";\n                var nbb_formatted_code = \"import numpy as np\\nfrom io import StringIO\\n\\ninput_string = \\\"\\\"\\\"\\n25 2 50 1 500 127900\\n39 3 10 1 1000 222100\\n13 2 13 1 1000 143750\\n82 5 20 2 120 268000\\n130 6 10 2 600 460700\\n115 6 10 1 550 407000\\n\\\"\\\"\\\"\\n\\nnp.set_printoptions(\\n    precision=1\\n)  # this just changes the output settings for easier reading\\n\\n\\ndef fit_model(input_file):\\n    # read the data in and fit it. the values below are placeholder values\\n    c = np.asarray([])  # coefficients of the linear regression\\n    x = np.asarray([])  # input data to the linear regression\\n    # This will return a numpy array that contains the feature data in the columns preceding the last one,\\n    # and the price data in the last column.\\n    data = np.genfromtxt(input_file, skip_header=1)\\n    x = data[:, :-1]\\n    y = data[:, -1]\\n    c = np.linalg.lstsq(x, y)[0]\\n    print(c)\\n    print(x @ c)\\n\\n\\n# simulate reading a file\\ninput_file = StringIO(input_string)\\nfit_model(input_file)\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            "
     },
     "metadata": {}
    }
   ],
   "source": [
    "import numpy as np\n",
    "from io import StringIO\n",
    "\n",
    "input_string = \"\"\"\n",
    "25 2 50 1 500 127900\n",
    "39 3 10 1 1000 222100\n",
    "13 2 13 1 1000 143750\n",
    "82 5 20 2 120 268000\n",
    "130 6 10 2 600 460700\n",
    "115 6 10 1 550 407000\n",
    "\"\"\"\n",
    "\n",
    "np.set_printoptions(\n",
    "    precision=1\n",
    ")  # this just changes the output settings for easier reading\n",
    "\n",
    "\n",
    "def fit_model(input_file):\n",
    "    # read the data in and fit it. the values below are placeholder values\n",
    "    c = np.asarray([])  # coefficients of the linear regression\n",
    "    x = np.asarray([])  # input data to the linear regression\n",
    "    # This will return a numpy array that contains the feature data in the columns preceding the last one,\n",
    "    # and the price data in the last column.\n",
    "    data = np.genfromtxt(input_file, skip_header=1)\n",
    "    x = data[:, :-1]\n",
    "    y = data[:, -1]\n",
    "    c = np.linalg.lstsq(x, y)[0]\n",
    "    print(c)\n",
    "    print(x @ c)\n",
    "\n",
    "\n",
    "# simulate reading a file\n",
    "input_file = StringIO(input_string)\n",
    "fit_model(input_file)"
   ]
  },
  {
   "source": [
    "### Exercise 14: Training data vs test data\n",
    "\n",
    "### -- Advanced\n",
    "\n",
    "Write a program that reads data about one set of cabins (training data), estimates linear regression coefficients based on it, then reads data about another set of cabins (test data), and predicts the prices in it. Note that both data sets contain the actual prices, but the program should ignore the prices in the second set. They are given only for comparison.\n",
    "\n",
    "You can read the data into the program the same way as in the previous exercise.\n",
    "\n",
    "You should then separate the feature and price data that you have just read from the file into two separate arrays names `x_train` and `y_train`, so that you can use them as argument to `np.linalg.lstsq`.\n",
    "\n",
    "The program should work even if the number of features used to describe the cabins differs from five (as long as the same number of features are given in each file).\n",
    "\n",
    "The output should be the set of coefficients for the linear regression and the predicted prices for the second set of cabins."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[2989.6  800.6  -44.8 3890.8   99.8]\n",
      "[198102.4 289108.3]\n",
      "<ipython-input-45-3ec61d2d8b33>:32: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  c = np.linalg.lstsq(x_train, y_train)[0]\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.Javascript object>",
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 45;\n                var nbb_unformatted_code = \"import numpy as np\\nfrom io import StringIO\\n\\n\\ntrain_string = '''\\n25 2 50 1 500 127900\\n39 3 10 1 1000 222100\\n13 2 13 1 1000 143750\\n82 5 20 2 120 268000\\n130 6 10 2 600 460700\\n115 6 10 1 550 407000\\n'''\\n\\ntest_string = '''\\n36 3 15 1 850 196000\\n75 5 18 2 540 290000\\n'''\\n\\ndef main():\\n    np.set_printoptions(precision=1)    # this just changes the output settings for easier reading\\n\\n    # read in the training data and separate it to x_train and y_train\\n    #   simulate reading a file\\n    input_train, input_test = StringIO(train_string), StringIO(test_string)\\n    data_train, data_test = np.genfromtxt(input_train, skip_header=1), np.genfromtxt(input_test, skip_header=1)\\n    \\n    x_train = data_train[:, :-1]\\n    y_train = data_train[:, -1]\\n     \\n    # fit a linear regression model to the data and get the coefficients\\n    c = np.asarray([])\\n    c = np.linalg.lstsq(x_train, y_train)[0]\\n\\n    # read in the test data and separate x_test from it\\n    x_test = np.asarray([])\\n    x_test = data_test[:, :-1]\\n\\n    # print out the linear regression coefficients\\n    print(c)\\n\\n    # this will print out the predicted prics for the two new cabins in the test data set\\n    print(x_test @ c)\\n\\n\\nmain()\";\n                var nbb_formatted_code = \"import numpy as np\\nfrom io import StringIO\\n\\n\\ntrain_string = \\\"\\\"\\\"\\n25 2 50 1 500 127900\\n39 3 10 1 1000 222100\\n13 2 13 1 1000 143750\\n82 5 20 2 120 268000\\n130 6 10 2 600 460700\\n115 6 10 1 550 407000\\n\\\"\\\"\\\"\\n\\ntest_string = \\\"\\\"\\\"\\n36 3 15 1 850 196000\\n75 5 18 2 540 290000\\n\\\"\\\"\\\"\\n\\n\\ndef main():\\n    np.set_printoptions(\\n        precision=1\\n    )  # this just changes the output settings for easier reading\\n\\n    # read in the training data and separate it to x_train and y_train\\n    #   simulate reading a file\\n    input_train, input_test = StringIO(train_string), StringIO(test_string)\\n    data_train, data_test = np.genfromtxt(input_train, skip_header=1), np.genfromtxt(\\n        input_test, skip_header=1\\n    )\\n\\n    x_train = data_train[:, :-1]\\n    y_train = data_train[:, -1]\\n\\n    # fit a linear regression model to the data and get the coefficients\\n    c = np.asarray([])\\n    c = np.linalg.lstsq(x_train, y_train)[0]\\n\\n    # read in the test data and separate x_test from it\\n    x_test = np.asarray([])\\n    x_test = data_test[:, :-1]\\n\\n    # print out the linear regression coefficients\\n    print(c)\\n\\n    # this will print out the predicted prics for the two new cabins in the test data set\\n    print(x_test @ c)\\n\\n\\nmain()\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            "
     },
     "metadata": {}
    }
   ],
   "source": [
    "import numpy as np\n",
    "from io import StringIO\n",
    "\n",
    "\n",
    "train_string = \"\"\"\n",
    "25 2 50 1 500 127900\n",
    "39 3 10 1 1000 222100\n",
    "13 2 13 1 1000 143750\n",
    "82 5 20 2 120 268000\n",
    "130 6 10 2 600 460700\n",
    "115 6 10 1 550 407000\n",
    "\"\"\"\n",
    "\n",
    "test_string = \"\"\"\n",
    "36 3 15 1 850 196000\n",
    "75 5 18 2 540 290000\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def main():\n",
    "    np.set_printoptions(\n",
    "        precision=1\n",
    "    )  # this just changes the output settings for easier reading\n",
    "\n",
    "    # read in the training data and separate it to x_train and y_train\n",
    "    #   simulate reading a file\n",
    "    input_train, input_test = StringIO(train_string), StringIO(test_string)\n",
    "    data_train, data_test = np.genfromtxt(input_train, skip_header=1), np.genfromtxt(\n",
    "        input_test, skip_header=1\n",
    "    )\n",
    "\n",
    "    x_train = data_train[:, :-1]\n",
    "    y_train = data_train[:, -1]\n",
    "\n",
    "    # fit a linear regression model to the data and get the coefficients\n",
    "    c = np.asarray([])\n",
    "    c = np.linalg.lstsq(x_train, y_train)[0]\n",
    "\n",
    "    # read in the test data and separate x_test from it\n",
    "    x_test = np.asarray([])\n",
    "    x_test = data_test[:, :-1]\n",
    "\n",
    "    # print out the linear regression coefficients\n",
    "    print(c)\n",
    "\n",
    "    # this will print out the predicted prics for the two new cabins in the test data set\n",
    "    print(x_test @ c)\n",
    "\n",
    "\n",
    "main()"
   ]
  },
  {
   "source": [
    "## II.The nearest neighbor method"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Exercise 15: Vector distances\n",
    "\n",
    "### -- Advanced\n",
    "\n",
    "You are given an array x_train with multiple input vectors (the \"training data\") and another array x_test with one more input vector (the \"test data\"). Find the vector in x_train that is most similar to the vector in x_test. In other words, find the nearest neighbor of the test data point x_test.\n",
    "\n",
    "The code template gives the function dist to calculate the distance between any two vectors. What you need to add is the implementation of the function nearest that takes the arrays x_train and x_test and prints the index (as an integer between 0, ..., len(x_train)-1) of the nearest neighbor."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "3\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.Javascript object>",
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 46;\n                var nbb_unformatted_code = \"import numpy as np\\n\\nx_train = np.random.rand(10, 3)   # generate 10 random vectors of dimension 3\\nx_test = np.random.rand(3)        # generate one more random vector of the same dimension\\n\\ndef dist(a, b):\\n    sum = 0\\n    for ai, bi in zip(a, b):\\n        sum = sum + (ai - bi)**2\\n    return np.sqrt(sum)\\n    \\ndef nearest(x_train, x_test):\\n    nearest = -1\\n    min_distance = np.Inf\\n    # add a loop here that goes through all the vectors in x_train and finds the one that\\n    # is nearest to x_test. return the index (between 0, ..., len(x_train)-1) of the nearest\\n    # neighbor\\n    for i, x in enumerate(x_train):\\n        distance = dist(x, x_test)\\n        if distance < min_distance:\\n            min_distance = distance\\n            nearest = i\\n    print(nearest)\\n\\nnearest(x_train, x_test)\";\n                var nbb_formatted_code = \"import numpy as np\\n\\nx_train = np.random.rand(10, 3)  # generate 10 random vectors of dimension 3\\nx_test = np.random.rand(3)  # generate one more random vector of the same dimension\\n\\n\\ndef dist(a, b):\\n    sum = 0\\n    for ai, bi in zip(a, b):\\n        sum = sum + (ai - bi) ** 2\\n    return np.sqrt(sum)\\n\\n\\ndef nearest(x_train, x_test):\\n    nearest = -1\\n    min_distance = np.Inf\\n    # add a loop here that goes through all the vectors in x_train and finds the one that\\n    # is nearest to x_test. return the index (between 0, ..., len(x_train)-1) of the nearest\\n    # neighbor\\n    for i, x in enumerate(x_train):\\n        distance = dist(x, x_test)\\n        if distance < min_distance:\\n            min_distance = distance\\n            nearest = i\\n    print(nearest)\\n\\n\\nnearest(x_train, x_test)\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            "
     },
     "metadata": {}
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "x_train = np.random.rand(10, 3)  # generate 10 random vectors of dimension 3\n",
    "x_test = np.random.rand(3)  # generate one more random vector of the same dimension\n",
    "\n",
    "\n",
    "def dist(a, b):\n",
    "    sum = 0\n",
    "    for ai, bi in zip(a, b):\n",
    "        sum = sum + (ai - bi) ** 2\n",
    "    return np.sqrt(sum)\n",
    "\n",
    "\n",
    "def nearest(x_train, x_test):\n",
    "    nearest = -1\n",
    "    min_distance = np.Inf\n",
    "    # add a loop here that goes through all the vectors in x_train and finds the one that\n",
    "    # is nearest to x_test. return the index (between 0, ..., len(x_train)-1) of the nearest\n",
    "    # neighbor\n",
    "    for i, x in enumerate(x_train):\n",
    "        distance = dist(x, x_test)\n",
    "        if distance < min_distance:\n",
    "            min_distance = distance\n",
    "            nearest = i\n",
    "    print(nearest)\n",
    "\n",
    "\n",
    "nearest(x_train, x_test)"
   ]
  },
  {
   "source": [
    "### Exercise 16: Nearest neighbor\n",
    "\n",
    "### --\n",
    "\n",
    "In the basic nearest neighbor classifier, the only thing that matters is the class label of the nearest neighbor. But the nearest neighbor may sometimes be noisy or otherwise misleading. Therefore, it may be better to also consider the other nearby data points in addition to the nearest neighbor.\n",
    "\n",
    "This idea leads us to the so called k-nearest neighbor method, where we consider all the k nearest neighbors. If k=3, for example, we'd take the three nearest points and choose the class label based on the majority class among them.\n",
    "\n",
    "The program below uses the library **sklearn** to create random data. Our input variable X has two features (compare to, say, cabin size and cabin price) and our target variable y is binary: it is either 0 or 1 (again think, for example, \"is the cabin awesome or not.\")\n",
    "\n",
    "Complete the following program so that it finds the three nearest data points (k=3) for each of the test data points and classifies them based on the majority class among the neighbors. Currently it generates the random data, splits it into training and test sets, calculates the distances between each test set items and the training set items, but it fails to classify the test set items according to the correct class, setting them all to belong to class 0. Instead of looking at just the nearest neighbor's class, it should use three neighbors and pick the majority class (the most common) class among the three neighbours, and use that as the class for the test item."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[0 1]\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.Javascript object>",
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 47;\n                var nbb_unformatted_code = \"## INTERM\\nimport numpy as np\\nfrom sklearn.datasets import make_blobs\\nfrom sklearn.preprocessing import MinMaxScaler\\nfrom sklearn.model_selection import train_test_split\\n\\n\\n# create random data with two classes\\nX, y = make_blobs(n_samples=16, n_features=2, centers=2, center_box=(-2, 2))\\n\\n# scale the data so that all values are between 0.0 and 1.0\\nX = MinMaxScaler().fit_transform(X)\\n\\n# split two data points from the data as test data and\\n# use the remaining n-2 points as the training data\\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=2)\\n\\n# place-holder for the predicted classes\\ny_predict = np.empty(len(y_test), dtype=np.int64)\\n\\n# produce line segments that connect the test data points\\n# to the nearest neighbors for drawing the chart\\nlines = []\\n\\n\\n# distance function\\ndef dist(a, b):\\n    sum = 0\\n    for ai, bi in zip(a, b):\\n        sum = sum + (ai - bi)**2\\n    return np.sqrt(sum)\\n\\n\\ndef main(X_train, X_test, y_train, y_test):\\n\\n    global y_predict\\n    global lines\\n    \\n    # process each of the test data points\\n    for i, test_item in enumerate(X_test):\\n        # calculate the distances to all training points\\n        distances = [dist(train_item, test_item) for train_item in X_train]\\n\\n        # find the index of the nearest neighbor\\n        nearest = np.argmin(distances)\\n\\n        # create a line connecting the points for the chart\\n        lines.append(np.stack((test_item, X_train[nearest])))\\n\\n        # add your code here:\\n        # y_predict[i] = 0          # this just classifies everything as 0 \\n        y_predict[i] = y_train[nearest]\\n\\n    print(y_predict)\\n\\n\\nmain(X_train, X_test, y_train, y_test)\";\n                var nbb_formatted_code = \"## INTERM\\nimport numpy as np\\nfrom sklearn.datasets import make_blobs\\nfrom sklearn.preprocessing import MinMaxScaler\\nfrom sklearn.model_selection import train_test_split\\n\\n\\n# create random data with two classes\\nX, y = make_blobs(n_samples=16, n_features=2, centers=2, center_box=(-2, 2))\\n\\n# scale the data so that all values are between 0.0 and 1.0\\nX = MinMaxScaler().fit_transform(X)\\n\\n# split two data points from the data as test data and\\n# use the remaining n-2 points as the training data\\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=2)\\n\\n# place-holder for the predicted classes\\ny_predict = np.empty(len(y_test), dtype=np.int64)\\n\\n# produce line segments that connect the test data points\\n# to the nearest neighbors for drawing the chart\\nlines = []\\n\\n\\n# distance function\\ndef dist(a, b):\\n    sum = 0\\n    for ai, bi in zip(a, b):\\n        sum = sum + (ai - bi) ** 2\\n    return np.sqrt(sum)\\n\\n\\ndef main(X_train, X_test, y_train, y_test):\\n\\n    global y_predict\\n    global lines\\n\\n    # process each of the test data points\\n    for i, test_item in enumerate(X_test):\\n        # calculate the distances to all training points\\n        distances = [dist(train_item, test_item) for train_item in X_train]\\n\\n        # find the index of the nearest neighbor\\n        nearest = np.argmin(distances)\\n\\n        # create a line connecting the points for the chart\\n        lines.append(np.stack((test_item, X_train[nearest])))\\n\\n        # add your code here:\\n        # y_predict[i] = 0          # this just classifies everything as 0\\n        y_predict[i] = y_train[nearest]\\n\\n    print(y_predict)\\n\\n\\nmain(X_train, X_test, y_train, y_test)\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            "
     },
     "metadata": {}
    }
   ],
   "source": [
    "## INTERM\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# create random data with two classes\n",
    "X, y = make_blobs(n_samples=16, n_features=2, centers=2, center_box=(-2, 2))\n",
    "\n",
    "# scale the data so that all values are between 0.0 and 1.0\n",
    "X = MinMaxScaler().fit_transform(X)\n",
    "\n",
    "# split two data points from the data as test data and\n",
    "# use the remaining n-2 points as the training data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=2)\n",
    "\n",
    "# place-holder for the predicted classes\n",
    "y_predict = np.empty(len(y_test), dtype=np.int64)\n",
    "\n",
    "# produce line segments that connect the test data points\n",
    "# to the nearest neighbors for drawing the chart\n",
    "lines = []\n",
    "\n",
    "\n",
    "# distance function\n",
    "def dist(a, b):\n",
    "    sum = 0\n",
    "    for ai, bi in zip(a, b):\n",
    "        sum = sum + (ai - bi) ** 2\n",
    "    return np.sqrt(sum)\n",
    "\n",
    "\n",
    "def main(X_train, X_test, y_train, y_test):\n",
    "\n",
    "    global y_predict\n",
    "    global lines\n",
    "\n",
    "    # process each of the test data points\n",
    "    for i, test_item in enumerate(X_test):\n",
    "        # calculate the distances to all training points\n",
    "        distances = [dist(train_item, test_item) for train_item in X_train]\n",
    "\n",
    "        # find the index of the nearest neighbor\n",
    "        nearest = np.argmin(distances)\n",
    "\n",
    "        # create a line connecting the points for the chart\n",
    "        lines.append(np.stack((test_item, X_train[nearest])))\n",
    "\n",
    "        # add your code here:\n",
    "        # y_predict[i] = 0          # this just classifies everything as 0\n",
    "        y_predict[i] = y_train[nearest]\n",
    "\n",
    "    print(y_predict)\n",
    "\n",
    "\n",
    "main(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[0 1]\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.Javascript object>",
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 48;\n                var nbb_unformatted_code = \"## ADVANCED\\n\\nimport numpy as np\\nfrom sklearn.datasets import make_blobs\\nfrom sklearn.preprocessing import MinMaxScaler\\nfrom sklearn.model_selection import train_test_split\\n\\n\\n# create random data with two classes\\nX, Y = make_blobs(n_samples=16, n_features=2, centers=2, center_box=(-2, 2))\\n\\n# scale the data so that all values are between 0.0 and 1.0\\nX = MinMaxScaler().fit_transform(X)\\n\\n# split two data points from the data as test data and\\n# use the remaining n-2 points as the training data\\nX_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=2)\\n\\n# place-holder for the predicted classes\\ny_predict = np.empty(len(y_test), dtype=np.int64)\\n\\n# produce line segments that connect the test data points\\n# to the nearest neighbors for drawing the chart\\nlines = []\\n\\n# distance function\\ndef dist(a, b):\\n    sum = 0\\n    for ai, bi in zip(a, b):\\n        sum = sum + (ai - bi)**2\\n    return np.sqrt(sum)\\n\\n\\ndef main(X_train, X_test, y_train, y_test):\\n\\n    global y_predict\\n    global lines\\n\\n    k = 3    # classify our test items based on the classes of 3 nearest neighbors\\n\\n    # process each of the test data points\\n    for i, test_item in enumerate(X_test):\\n        # calculate the distances to all training points\\n        distances = [dist(train_item, test_item) for train_item in X_train]\\n\\n        # add your code here\\n        #nearest = np.argmin(distances)       # this just finds the nearest neighbour (so k=1)\\n        y_train_k = [y_train[i] for i in np.argpartition(distances,k)[:k]]\\n        nearest = np.bincount(y_train_k).argmax()\\n\\n        # create a line connecting the points for the chart\\n        # you may change this to do the same for all the k nearest neigbhors if you like\\n        # but it will not be checked in the tests\\n        lines.append(np.stack((test_item, X_train[nearest])))\\n\\n        y_predict[i] = nearest     # 0          # this just classifies everything as 0\\n    \\n    print(y_predict)\\n\\nmain(X_train, X_test, y_train, y_test)\";\n                var nbb_formatted_code = \"## ADVANCED\\n\\nimport numpy as np\\nfrom sklearn.datasets import make_blobs\\nfrom sklearn.preprocessing import MinMaxScaler\\nfrom sklearn.model_selection import train_test_split\\n\\n\\n# create random data with two classes\\nX, Y = make_blobs(n_samples=16, n_features=2, centers=2, center_box=(-2, 2))\\n\\n# scale the data so that all values are between 0.0 and 1.0\\nX = MinMaxScaler().fit_transform(X)\\n\\n# split two data points from the data as test data and\\n# use the remaining n-2 points as the training data\\nX_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=2)\\n\\n# place-holder for the predicted classes\\ny_predict = np.empty(len(y_test), dtype=np.int64)\\n\\n# produce line segments that connect the test data points\\n# to the nearest neighbors for drawing the chart\\nlines = []\\n\\n# distance function\\ndef dist(a, b):\\n    sum = 0\\n    for ai, bi in zip(a, b):\\n        sum = sum + (ai - bi) ** 2\\n    return np.sqrt(sum)\\n\\n\\ndef main(X_train, X_test, y_train, y_test):\\n\\n    global y_predict\\n    global lines\\n\\n    k = 3  # classify our test items based on the classes of 3 nearest neighbors\\n\\n    # process each of the test data points\\n    for i, test_item in enumerate(X_test):\\n        # calculate the distances to all training points\\n        distances = [dist(train_item, test_item) for train_item in X_train]\\n\\n        # add your code here\\n        # nearest = np.argmin(distances)       # this just finds the nearest neighbour (so k=1)\\n        y_train_k = [y_train[i] for i in np.argpartition(distances, k)[:k]]\\n        nearest = np.bincount(y_train_k).argmax()\\n\\n        # create a line connecting the points for the chart\\n        # you may change this to do the same for all the k nearest neigbhors if you like\\n        # but it will not be checked in the tests\\n        lines.append(np.stack((test_item, X_train[nearest])))\\n\\n        y_predict[i] = nearest  # 0          # this just classifies everything as 0\\n\\n    print(y_predict)\\n\\n\\nmain(X_train, X_test, y_train, y_test)\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            "
     },
     "metadata": {}
    }
   ],
   "source": [
    "## ADVANCED\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# create random data with two classes\n",
    "X, Y = make_blobs(n_samples=16, n_features=2, centers=2, center_box=(-2, 2))\n",
    "\n",
    "# scale the data so that all values are between 0.0 and 1.0\n",
    "X = MinMaxScaler().fit_transform(X)\n",
    "\n",
    "# split two data points from the data as test data and\n",
    "# use the remaining n-2 points as the training data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=2)\n",
    "\n",
    "# place-holder for the predicted classes\n",
    "y_predict = np.empty(len(y_test), dtype=np.int64)\n",
    "\n",
    "# produce line segments that connect the test data points\n",
    "# to the nearest neighbors for drawing the chart\n",
    "lines = []\n",
    "\n",
    "# distance function\n",
    "def dist(a, b):\n",
    "    sum = 0\n",
    "    for ai, bi in zip(a, b):\n",
    "        sum = sum + (ai - bi) ** 2\n",
    "    return np.sqrt(sum)\n",
    "\n",
    "\n",
    "def main(X_train, X_test, y_train, y_test):\n",
    "\n",
    "    global y_predict\n",
    "    global lines\n",
    "\n",
    "    k = 3  # classify our test items based on the classes of 3 nearest neighbors\n",
    "\n",
    "    # process each of the test data points\n",
    "    for i, test_item in enumerate(X_test):\n",
    "        # calculate the distances to all training points\n",
    "        distances = [dist(train_item, test_item) for train_item in X_train]\n",
    "\n",
    "        # add your code here\n",
    "        # nearest = np.argmin(distances)       # this just finds the nearest neighbour (so k=1)\n",
    "        y_train_k = [y_train[i] for i in np.argpartition(distances, k)[:k]]\n",
    "        nearest = np.bincount(y_train_k).argmax()\n",
    "\n",
    "        # create a line connecting the points for the chart\n",
    "        # you may change this to do the same for all the k nearest neigbhors if you like\n",
    "        # but it will not be checked in the tests\n",
    "        lines.append(np.stack((test_item, X_train[nearest])))\n",
    "\n",
    "        y_predict[i] = nearest  # 0          # this just classifies everything as 0\n",
    "\n",
    "    print(y_predict)\n",
    "\n",
    "\n",
    "main(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "source": [
    "## III. Working with text"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Exercise 17: Bag of words\n",
    "\n",
    "### -- Advanced\n",
    "\n",
    "Your task is to write a program that calculates the distances (or differences) between every pair of lines in the This Little Piggy rhyme and find the most similar pair. Use the Manhattan distance as your distance metric.\n",
    "\n",
    "You can start by building a numpy array with all the distances. Notice that the diagonal elements (elements at positions [i, j] with i=j) will be equal to zero because each row is equal to itself. To avoid selecting them, you can assign the value np.inf (the maximum possible floating point value). Note that to do this, it's necessary to make sure the type of the array is float. A convenient and fast way to get the index of the element with the lowest value in a 2D array (or in fact, any dimension) is by the function\n",
    "np.unravel_index(np.argmin(dist), dist.shape))\n",
    "where dist is the array. This will return the index of the lowest valued element as a list of length two (assuming the array is two-dimensional)."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(2, 3)\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.Javascript object>",
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 49;\n                var nbb_unformatted_code = \"import numpy as np\\n\\ndata = [[1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1],\\n        [1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1],\\n        [1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1],\\n        [1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1],\\n        [1, 1, 1, 0, 1, 3, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1]]\\n\\ndef distance(row1, row2):\\n    return sum(abs(i-j) for i, j in zip(row1, row2))\\n\\ndef find_nearest_pair(data):\\n    N = len(data)\\n    dist = np.empty((N, N), dtype=float)\\n\\n    #for i in range(N):\\n    #  for j in range(N):\\n    #    dist[i, j] = np.inf if i == j else distance(data[i], data[j])\\n    # shorter version:\\n    dist = np.array([np.array([distance(sent1, sent2) if sent1 != sent2 else np.inf for sent1 in data]) for sent2 in data])\\n    \\n    print(np.unravel_index(np.argmin(dist), dist.shape))\\n\\nfind_nearest_pair(data)\";\n                var nbb_formatted_code = \"import numpy as np\\n\\ndata = [\\n    [1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1],\\n    [1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1],\\n    [1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1],\\n    [1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1],\\n    [1, 1, 1, 0, 1, 3, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1],\\n]\\n\\n\\ndef distance(row1, row2):\\n    return sum(abs(i - j) for i, j in zip(row1, row2))\\n\\n\\ndef find_nearest_pair(data):\\n    N = len(data)\\n    dist = np.empty((N, N), dtype=float)\\n\\n    # for i in range(N):\\n    #  for j in range(N):\\n    #    dist[i, j] = np.inf if i == j else distance(data[i], data[j])\\n    # shorter version:\\n    dist = np.array(\\n        [\\n            np.array(\\n                [distance(sent1, sent2) if sent1 != sent2 else np.inf for sent1 in data]\\n            )\\n            for sent2 in data\\n        ]\\n    )\\n\\n    print(np.unravel_index(np.argmin(dist), dist.shape))\\n\\n\\nfind_nearest_pair(data)\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            "
     },
     "metadata": {}
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "data = [\n",
    "    [1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1],\n",
    "    [1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1],\n",
    "    [1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1],\n",
    "    [1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1],\n",
    "    [1, 1, 1, 0, 1, 3, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1],\n",
    "]\n",
    "\n",
    "\n",
    "def distance(row1, row2):\n",
    "    return sum(abs(i - j) for i, j in zip(row1, row2))\n",
    "\n",
    "\n",
    "def find_nearest_pair(data):\n",
    "    N = len(data)\n",
    "    dist = np.empty((N, N), dtype=float)\n",
    "\n",
    "    # for i in range(N):\n",
    "    #  for j in range(N):\n",
    "    #    dist[i, j] = np.inf if i == j else distance(data[i], data[j])\n",
    "    # shorter version:\n",
    "    dist = np.array(\n",
    "        [\n",
    "            np.array(\n",
    "                [distance(sent1, sent2) if sent1 != sent2 else np.inf for sent1 in data]\n",
    "            )\n",
    "            for sent2 in data\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    print(np.unravel_index(np.argmin(dist), dist.shape))\n",
    "\n",
    "\n",
    "find_nearest_pair(data)"
   ]
  },
  {
   "source": [
    "### Exercise 18: TF-IDF\n",
    "\n",
    "### -- Intermediate\n",
    "\n",
    "Modify the following program to print out the tf-idf values for each document and each word. The following code calculates the tf and df values, so you'll just need to combine them according to the correct formula. There are three documents (sentences) and a total of eight terms (unique words), so the output should be three lists of eight tf-idf values each."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[0.19084850188786498, 0.09542425094393249, 0.03521825181113625, 0.03521825181113625]\n[0.04402281476392031, 0.04402281476392031, 0.04402281476392031, 0.11928031367991561]\n[0.04402281476392031, 0.04402281476392031, 0.04402281476392031, 0.11928031367991561]\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.Javascript object>",
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 50;\n                var nbb_unformatted_code = \"# Modify the following program to print out the tf-idf values for each document and each word.\\n\\n# DATA BLOCK\\n\\ntext = '''he really really loves coffee\\nmy sister dislikes coffee\\nmy sister loves tea'''\\n\\nimport math\\n\\ndef main(text):\\n    # split the text first into lines and then into lists of words\\n    docs = [line.split() for line in text.splitlines()]\\n\\n    N = len(docs)\\n\\n    # create the vocabulary: the list of words that appear at least once\\n    vocabulary = list(set(text.split()))\\n\\n    df = {}\\n    tf = {}\\n    for word in vocabulary:\\n        # tf: number of occurrences of word w in document divided by document length\\n        # note: tf[word] will be a list containing the tf of each word for each document\\n        # for example tf['he'][0] contains the term frequence of the word 'he' in the first\\n        # document\\n        tf[word] = [doc.count(word)/len(doc) for doc in docs]\\n\\n        # df: number of documents containing word w\\n        df[word] = sum([word in doc for doc in docs])/N\\n\\n    # loop through documents to calculate the tf-idf values\\n    for doc_index, doc in enumerate(docs):\\n        tfidf = []\\n        for word in vocabulary:\\n            to_append = tf[word][doc_index] * math.log(1 / df[word], 10)\\n            if to_append != 0:\\n                tfidf.append(to_append) \\n        print(tfidf)\\n\\nmain(text)\";\n                var nbb_formatted_code = \"# Modify the following program to print out the tf-idf values for each document and each word.\\n\\n# DATA BLOCK\\n\\ntext = \\\"\\\"\\\"he really really loves coffee\\nmy sister dislikes coffee\\nmy sister loves tea\\\"\\\"\\\"\\n\\nimport math\\n\\n\\ndef main(text):\\n    # split the text first into lines and then into lists of words\\n    docs = [line.split() for line in text.splitlines()]\\n\\n    N = len(docs)\\n\\n    # create the vocabulary: the list of words that appear at least once\\n    vocabulary = list(set(text.split()))\\n\\n    df = {}\\n    tf = {}\\n    for word in vocabulary:\\n        # tf: number of occurrences of word w in document divided by document length\\n        # note: tf[word] will be a list containing the tf of each word for each document\\n        # for example tf['he'][0] contains the term frequence of the word 'he' in the first\\n        # document\\n        tf[word] = [doc.count(word) / len(doc) for doc in docs]\\n\\n        # df: number of documents containing word w\\n        df[word] = sum([word in doc for doc in docs]) / N\\n\\n    # loop through documents to calculate the tf-idf values\\n    for doc_index, doc in enumerate(docs):\\n        tfidf = []\\n        for word in vocabulary:\\n            to_append = tf[word][doc_index] * math.log(1 / df[word], 10)\\n            if to_append != 0:\\n                tfidf.append(to_append)\\n        print(tfidf)\\n\\n\\nmain(text)\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            "
     },
     "metadata": {}
    }
   ],
   "source": [
    "# Modify the following program to print out the tf-idf values for each document and each word.\n",
    "\n",
    "# DATA BLOCK\n",
    "\n",
    "text = \"\"\"he really really loves coffee\n",
    "my sister dislikes coffee\n",
    "my sister loves tea\"\"\"\n",
    "\n",
    "import math\n",
    "\n",
    "\n",
    "def main(text):\n",
    "    # split the text first into lines and then into lists of words\n",
    "    docs = [line.split() for line in text.splitlines()]\n",
    "\n",
    "    N = len(docs)\n",
    "\n",
    "    # create the vocabulary: the list of words that appear at least once\n",
    "    vocabulary = list(set(text.split()))\n",
    "\n",
    "    df = {}\n",
    "    tf = {}\n",
    "    for word in vocabulary:\n",
    "        # tf: number of occurrences of word w in document divided by document length\n",
    "        # note: tf[word] will be a list containing the tf of each word for each document\n",
    "        # for example tf['he'][0] contains the term frequence of the word 'he' in the first\n",
    "        # document\n",
    "        tf[word] = [doc.count(word) / len(doc) for doc in docs]\n",
    "\n",
    "        # df: number of documents containing word w\n",
    "        df[word] = sum([word in doc for doc in docs]) / N\n",
    "\n",
    "    # loop through documents to calculate the tf-idf values\n",
    "    for doc_index, doc in enumerate(docs):\n",
    "        tfidf = []\n",
    "        for word in vocabulary:\n",
    "            to_append = tf[word][doc_index] * math.log(1 / df[word], 10)\n",
    "            if to_append != 0:\n",
    "                tfidf.append(to_append)\n",
    "        print(tfidf)\n",
    "\n",
    "\n",
    "main(text)"
   ]
  },
  {
   "source": [
    "### -- Advanced\n",
    "\n",
    "Write a program that uses the tf-idf vectors to find the most similar pair of lines in a given data set. You can test your solution with the example text below. Note, however, that your solution will be tested on other data sets too, so make sure you don't make use of any special properties of the example data (like there being four lines of text)."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(0, 1)\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.Javascript object>",
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 51;\n                var nbb_unformatted_code = \"text = '''Humpty Dumpty sat on a wall\\nHumpty Dumpty had a great fall\\nall the king's horses and all the king's men\\ncouldn't put Humpty together again'''\\n\\nimport math\\nimport numpy as np\\n\\ndef main(text):\\n    # 1. split the text into words, and get a list of unique words that appear in it\\n    # a short one-liner to separate the text into sentences (with words lower-cased to make words equal \\n    # despite casing) can be done with \\n    # docs = [line.lower().split() for line in text.split('\\\\n')]\\n    text = text.lower()\\n    voc = list(set(text.split()))\\n    docs = [line.split() for line in text.split('\\\\n')]\\n\\n    # 2. go over each unique word and calculate its term frequency, and its document frequency\\n    # The document frequency of a word is the number of documents that contain at least one occurrence of the word\\n    tf = dict()\\n    df = dict()\\n    for word in voc:\\n        tf[word] = [doc.count(word)/len(doc) for doc in docs]\\n        df[word] = sum([word in doc for doc in docs])/len(docs)\\n\\n    # 3. after you have your term frequencies and document frequencies, go over each line in the text and \\n    # calculate its TF-IDF representation, which will be a vector\\n    tfdf = []   # TF-IDF vector or all documents\\n    for doc_index, doc in enumerate(docs):\\n        tfidf = []\\n        for word in voc:\\n            to_append = tf[word][doc_index] * math.log(1 / df[word], 10)\\n            # adding even 0 values as otherwise vectors have different length\\n            tfidf.append(to_append)\\n        tfdf.append(tfidf)\\n\\n    # 4. after you have calculated the TF-IDF representations for each line in the text, you need to\\n    # calculate the distances between each line to find which are the closest.\\n    def distance(row1, row2):\\n        return sum(abs(i-j) for i, j in zip(row1, row2))\\n\\n    def find_nearest_pair(data):\\n        N = len(data)\\n        dist = np.empty((N, N), dtype=float)\\n        # SAME: dist = np.array([np.array([distance(sent1, sent2) if sent1 != sent2 else np.inf for sent1 in data]) for sent2 in data])\\n        for i in range(N):\\n            for j in range(N):\\n                dist[i, j] = np.inf if i == j else distance(data[i], data[j])\\n        print(np.unravel_index(np.argmin(dist), dist.shape))\\n\\n\\n    find_nearest_pair(tfdf)\\n\\n\\nmain(text)\";\n                var nbb_formatted_code = \"text = \\\"\\\"\\\"Humpty Dumpty sat on a wall\\nHumpty Dumpty had a great fall\\nall the king's horses and all the king's men\\ncouldn't put Humpty together again\\\"\\\"\\\"\\n\\nimport math\\nimport numpy as np\\n\\n\\ndef main(text):\\n    # 1. split the text into words, and get a list of unique words that appear in it\\n    # a short one-liner to separate the text into sentences (with words lower-cased to make words equal\\n    # despite casing) can be done with\\n    # docs = [line.lower().split() for line in text.split('\\\\n')]\\n    text = text.lower()\\n    voc = list(set(text.split()))\\n    docs = [line.split() for line in text.split(\\\"\\\\n\\\")]\\n\\n    # 2. go over each unique word and calculate its term frequency, and its document frequency\\n    # The document frequency of a word is the number of documents that contain at least one occurrence of the word\\n    tf = dict()\\n    df = dict()\\n    for word in voc:\\n        tf[word] = [doc.count(word) / len(doc) for doc in docs]\\n        df[word] = sum([word in doc for doc in docs]) / len(docs)\\n\\n    # 3. after you have your term frequencies and document frequencies, go over each line in the text and\\n    # calculate its TF-IDF representation, which will be a vector\\n    tfdf = []  # TF-IDF vector or all documents\\n    for doc_index, doc in enumerate(docs):\\n        tfidf = []\\n        for word in voc:\\n            to_append = tf[word][doc_index] * math.log(1 / df[word], 10)\\n            # adding even 0 values as otherwise vectors have different length\\n            tfidf.append(to_append)\\n        tfdf.append(tfidf)\\n\\n    # 4. after you have calculated the TF-IDF representations for each line in the text, you need to\\n    # calculate the distances between each line to find which are the closest.\\n    def distance(row1, row2):\\n        return sum(abs(i - j) for i, j in zip(row1, row2))\\n\\n    def find_nearest_pair(data):\\n        N = len(data)\\n        dist = np.empty((N, N), dtype=float)\\n        # SAME: dist = np.array([np.array([distance(sent1, sent2) if sent1 != sent2 else np.inf for sent1 in data]) for sent2 in data])\\n        for i in range(N):\\n            for j in range(N):\\n                dist[i, j] = np.inf if i == j else distance(data[i], data[j])\\n        print(np.unravel_index(np.argmin(dist), dist.shape))\\n\\n    find_nearest_pair(tfdf)\\n\\n\\nmain(text)\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            "
     },
     "metadata": {}
    }
   ],
   "source": [
    "text = \"\"\"Humpty Dumpty sat on a wall\n",
    "Humpty Dumpty had a great fall\n",
    "all the king's horses and all the king's men\n",
    "couldn't put Humpty together again\"\"\"\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def main(text):\n",
    "    # 1. split the text into words, and get a list of unique words that appear in it\n",
    "    # a short one-liner to separate the text into sentences (with words lower-cased to make words equal\n",
    "    # despite casing) can be done with\n",
    "    # docs = [line.lower().split() for line in text.split('\\n')]\n",
    "    text = text.lower()\n",
    "    voc = list(set(text.split()))\n",
    "    docs = [line.split() for line in text.split(\"\\n\")]\n",
    "\n",
    "    # 2. go over each unique word and calculate its term frequency, and its document frequency\n",
    "    # The document frequency of a word is the number of documents that contain at least one occurrence of the word\n",
    "    tf = dict()\n",
    "    df = dict()\n",
    "    for word in voc:\n",
    "        tf[word] = [doc.count(word) / len(doc) for doc in docs]\n",
    "        df[word] = sum([word in doc for doc in docs]) / len(docs)\n",
    "\n",
    "    # 3. after you have your term frequencies and document frequencies, go over each line in the text and\n",
    "    # calculate its TF-IDF representation, which will be a vector\n",
    "    tfdf = []  # TF-IDF vector or all documents\n",
    "    for doc_index, doc in enumerate(docs):\n",
    "        tfidf = []\n",
    "        for word in voc:\n",
    "            to_append = tf[word][doc_index] * math.log(1 / df[word], 10)\n",
    "            # adding even 0 values as otherwise vectors have different length\n",
    "            tfidf.append(to_append)\n",
    "        tfdf.append(tfidf)\n",
    "\n",
    "    # 4. after you have calculated the TF-IDF representations for each line in the text, you need to\n",
    "    # calculate the distances between each line to find which are the closest.\n",
    "    def distance(row1, row2):\n",
    "        return sum(abs(i - j) for i, j in zip(row1, row2))\n",
    "\n",
    "    def find_nearest_pair(data):\n",
    "        N = len(data)\n",
    "        dist = np.empty((N, N), dtype=float)\n",
    "        # SAME: dist = np.array([np.array([distance(sent1, sent2) if sent1 != sent2 else np.inf for sent1 in data]) for sent2 in data])\n",
    "        for i in range(N):\n",
    "            for j in range(N):\n",
    "                dist[i, j] = np.inf if i == j else distance(data[i], data[j])\n",
    "        print(np.unravel_index(np.argmin(dist), dist.shape))\n",
    "\n",
    "    find_nearest_pair(tfdf)\n",
    "\n",
    "\n",
    "main(text)"
   ]
  },
  {
   "source": [
    "## IV.Overfitting"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Exercise 19: Looking out for overfitting\n",
    "### -- Advanced\n",
    "The program below uses the k-nearest neighbors algorithm. The idea is to not only look at the single nearest training data point (neighbor) but for example the five nearest points, if k=5. The normal nearest neighbor classifier amounts to using k=1.\n",
    "\n",
    "Write a program that does the classification for some value of k and prints out the training and testing accuracy.\n",
    "\n",
    "Hint: You can get the model accuracy for a given set using the function knn.score.\n",
    "\n",
    "Try different values of k to answer the questions below."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "training accuracy: 0.9253731343283582\ntesting accuracy: 0.9090909090909091\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.Javascript object>",
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 52;\n                var nbb_unformatted_code = \"from sklearn.neighbors import KNeighborsClassifier\\nfrom sklearn.datasets import make_moons\\nfrom sklearn.model_selection import train_test_split\\nimport numpy as np\\n\\n# do not edit this\\n# create fake data\\nx, y = make_moons(\\n    n_samples=500,  # the number of observations\\n    random_state=42,\\n    noise=0.3\\n)\\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.33, random_state=42)\\n\\n# Create a classifier and fit it to our data\\nknn = KNeighborsClassifier(n_neighbors=42)  # <-- that's the k!\\nknn.fit(x_train, y_train)\\n\\ntrain_acc = knn.score(x_train, y_train)\\ntest_acc = knn.score(x_test, y_test)\\nprint(f'training accuracy: {train_acc}')\\nprint(f'testing accuracy: {test_acc}')\";\n                var nbb_formatted_code = \"from sklearn.neighbors import KNeighborsClassifier\\nfrom sklearn.datasets import make_moons\\nfrom sklearn.model_selection import train_test_split\\nimport numpy as np\\n\\n# do not edit this\\n# create fake data\\nx, y = make_moons(\\n    n_samples=500, random_state=42, noise=0.3  # the number of observations\\n)\\nx_train, x_test, y_train, y_test = train_test_split(\\n    x, y, test_size=0.33, random_state=42\\n)\\n\\n# Create a classifier and fit it to our data\\nknn = KNeighborsClassifier(n_neighbors=42)  # <-- that's the k!\\nknn.fit(x_train, y_train)\\n\\ntrain_acc = knn.score(x_train, y_train)\\ntest_acc = knn.score(x_test, y_test)\\nprint(f\\\"training accuracy: {train_acc}\\\")\\nprint(f\\\"testing accuracy: {test_acc}\\\")\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            "
     },
     "metadata": {}
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "# do not edit this\n",
    "# create fake data\n",
    "x, y = make_moons(\n",
    "    n_samples=500, random_state=42, noise=0.3  # the number of observations\n",
    ")\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    x, y, test_size=0.33, random_state=42\n",
    ")\n",
    "\n",
    "# Create a classifier and fit it to our data\n",
    "knn = KNeighborsClassifier(n_neighbors=42)  # <-- that's the k!\n",
    "knn.fit(x_train, y_train)\n",
    "\n",
    "train_acc = knn.score(x_train, y_train)\n",
    "test_acc = knn.score(x_test, y_test)\n",
    "print(f\"training accuracy: {train_acc}\")\n",
    "print(f\"testing accuracy: {test_acc}\")"
   ]
  },
  {
   "source": [
    "**What would be a reasonable baseline accuracy your model should outperform in order for it to be considered useful?**\n",
    "\n",
    "- [x] 0.50\n",
    "- [ ] 0.25\n",
    "- [ ] any performance that is better than all wrong is enough as a baseline\n",
    "\n",
    "There are two classes, and the data points are evenly split among them. Assigning every point to either class, or picking a class randomly would result in a 50% accuracy.\n",
    "\n",
    "**Which of the following values of k do you think was \"best\"?**\n",
    "\n",
    "- [ ] the choice of k doesn't matter\n",
    "- [ ] k = 1\n",
    "- [ ] k = 250\n",
    "- [x] k = 42\n",
    "\n",
    "**Why?**\n",
    "\n",
    "- [ ] it gave the lowest training accuracy\n",
    "- [ ] it gave the highest training accuracy\n",
    "- [x] it gave the highest testing accuracy\n",
    "- [ ] it gave the lowest testing accuracy\n",
    "- [ ] the choice of k doesn't matter\n",
    "\n",
    "**Is it possible to have a higher test set accuracy than training set accuracy?**\n",
    "\n",
    "- [x] yes\n",
    "- [ ] no"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Neural networks"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## I.Logistic regression\n",
    "\n",
    "### Exercise 20: Logistic regression\n",
    "\n",
    "### -- Advanced\n",
    "\n",
    "You are given a set of three input values and you also have multiple alternative sets of three coefficients. Calculate the predicted output value using the linear formula combined with the logistic activation function.\n",
    "\n",
    "Do this with all the alternative sets of coefficients. Which of the coefficient sets yields the highest sigmoid output?"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.1544652650835347\n",
      "0.45016600268752216\n",
      "0.8455347349164652\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.Javascript object>",
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 53;\n                var nbb_unformatted_code = \"import math\\nimport numpy as np\\n\\nx = np.array([4, 3, 0])\\nc1 = np.array([-.5, .1, .08])\\nc2 = np.array([-.2, .2, .31])\\nc3 = np.array([.5, -.1, 2.53])\\n\\ndef sigmoid(z):\\n    # add your implementation of the sigmoid function here\\n    # Sigmoid function: s(z) = 1\\u00f7(1+exp(\\u2212z))\\n    z = -z  # exp(z) does not accept \\\"-z\\\" as argument\\n    print(1 / (1 + math.exp(z)))\\n\\n# calculate the output of the sigmoid for x with all three coefficients\\nsigmoid(x@c1)\\nsigmoid(x@c2)\\nsigmoid(x@c3)   # <-- this one\";\n                var nbb_formatted_code = \"import math\\nimport numpy as np\\n\\nx = np.array([4, 3, 0])\\nc1 = np.array([-0.5, 0.1, 0.08])\\nc2 = np.array([-0.2, 0.2, 0.31])\\nc3 = np.array([0.5, -0.1, 2.53])\\n\\n\\ndef sigmoid(z):\\n    # add your implementation of the sigmoid function here\\n    # Sigmoid function: s(z) = 1\\u00f7(1+exp(\\u2212z))\\n    z = -z  # exp(z) does not accept \\\"-z\\\" as argument\\n    print(1 / (1 + math.exp(z)))\\n\\n\\n# calculate the output of the sigmoid for x with all three coefficients\\nsigmoid(x @ c1)\\nsigmoid(x @ c2)\\nsigmoid(x @ c3)  # <-- this one\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            "
     },
     "metadata": {}
    }
   ],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "\n",
    "x = np.array([4, 3, 0])\n",
    "c1 = np.array([-0.5, 0.1, 0.08])\n",
    "c2 = np.array([-0.2, 0.2, 0.31])\n",
    "c3 = np.array([0.5, -0.1, 2.53])\n",
    "\n",
    "\n",
    "def sigmoid(z):\n",
    "    # add your implementation of the sigmoid function here\n",
    "    # Sigmoid function: s(z) = 1÷(1+exp(−z))\n",
    "    z = -z  # exp(z) does not accept \"-z\" as argument\n",
    "    print(1 / (1 + math.exp(z)))\n",
    "\n",
    "\n",
    "# calculate the output of the sigmoid for x with all three coefficients\n",
    "sigmoid(x @ c1)\n",
    "sigmoid(x @ c2)\n",
    "sigmoid(x @ c3)  # <-- this one"
   ]
  },
  {
   "source": [
    "## II.From logistic regression to neural networks\n",
    "\n",
    "### Exercise 21: Neural Networks\n",
    "\n",
    "### -- Advanced\n",
    "\n",
    "We have trained a simple neural network with a larger set of cabin price data. The network predicts the price of the cabin based on the attributes of the cabin. The network consists of an input layer with five nodes, a hidden layer with two nodes, a second hidden layer with two nodes, and finally an output layer with a single node. In addition, there is a single bias node for each hidden layer and the output layer.\n",
    "\n",
    "The program below uses the weights of this trained network to perform what is called a forward pass of the neural network. The forward pass is running the input variables through the neural network to obtain output, in this case the price of a cabin of given attributes.\n",
    "\n",
    "The program is incomplete though. The bias nodes are not used in the version below, and the activation functions for the hidden layers and the output layer have not been properly defined.\n",
    "\n",
    "Modify the program to use the bias nodes, and to use the ReLU activation function for the hidden nodes, and a linear (identity) activation for the output node. ReLU activation function returns either the input value of the function, or zero, whichever is the largest, and linear activation just returns the input as output. After these are done, get a prediction for the price of a cabin which is described by the following feature vector `[74, 5, 10, 2, 100]`."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[230008.7]\n[183615.4]\n[232721.4]\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.Javascript object>",
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 54;\n                var nbb_unformatted_code = \"import numpy as np\\n\\nw0 = np.array([[ 1.19627687e+01,  2.60163283e-01],\\n               [ 4.48832507e-01,  4.00666119e-01],\\n               [-2.75768443e-01,  3.43724167e-01],\\n                   [ 2.29138536e+01,  3.91783025e-01],\\n                   [-1.22397711e-02, -1.03029800e+00]])\\n\\nw1 = np.array([[11.5631751 , 11.87043684],\\n                   [-0.85735419,  0.27114237]])\\n\\nw2 = np.array([[11.04122165],\\n                   [10.44637262]])\\n\\nb0 = np.array([-4.21310294, -0.52664488])\\nb1 = np.array([-4.84067881, -4.53335139])\\nb2 = np.array([-7.52942418])\\n\\nx = np.array([[111, 13, 12, 1, 161],\\n                 [125, 13, 66, 1, 468],\\n                 [46, 6, 127, 2, 961],\\n                 [80, 9, 80, 2, 816],\\n                 [33, 10, 18, 2, 297],\\n                 [85, 9, 111, 3, 601],\\n                 [24, 10, 105, 2, 1072],\\n                 [31, 4, 66, 1, 417],\\n                 [56, 3, 60, 1, 36],\\n                 [49, 3, 147, 2, 179]])\\ny = np.array([335800., 379100., 118950., 247200., 107950., 266550.,  75850.,\\n                93300., 170650., 149000.])\\n\\n\\ndef hidden_activation(z):\\n    # ReLU activation. fix this!\\n    # ReLU activation function returns either the input value of the function, or zero, whichever is the largest\\n    return np.maximum(0, z)\\n\\ndef output_activation(z):\\n    # identity (linear) activation. fix this!\\n    # linear activation just returns the input as output\\n    return z\\n\\nx_test = [[72, 2, 25, 3, 450], [60, 3, 15, 1, 300], [74, 5, 10, 2, 100]]\\nfor item in x_test:\\n    h1_in = np.dot(item, w0) + b0 # this calculates the linear combination of inputs and weights. it is missing the bias term, fix it!\\n    h1_out = hidden_activation(h1_in) # apply activation function\\n    \\n    h2_in = np.dot(h1_out, w1) + b1 # the output of the previous layer is the input for this layer. it is missing the bias term, fix it!\\n    h2_out = hidden_activation(h2_in)\\n    \\n    out_in = np.dot(h2_out, w2) + b2\\n    out = output_activation(out_in)\\n    print(out)\";\n                var nbb_formatted_code = \"import numpy as np\\n\\nw0 = np.array(\\n    [\\n        [1.19627687e01, 2.60163283e-01],\\n        [4.48832507e-01, 4.00666119e-01],\\n        [-2.75768443e-01, 3.43724167e-01],\\n        [2.29138536e01, 3.91783025e-01],\\n        [-1.22397711e-02, -1.03029800e00],\\n    ]\\n)\\n\\nw1 = np.array([[11.5631751, 11.87043684], [-0.85735419, 0.27114237]])\\n\\nw2 = np.array([[11.04122165], [10.44637262]])\\n\\nb0 = np.array([-4.21310294, -0.52664488])\\nb1 = np.array([-4.84067881, -4.53335139])\\nb2 = np.array([-7.52942418])\\n\\nx = np.array(\\n    [\\n        [111, 13, 12, 1, 161],\\n        [125, 13, 66, 1, 468],\\n        [46, 6, 127, 2, 961],\\n        [80, 9, 80, 2, 816],\\n        [33, 10, 18, 2, 297],\\n        [85, 9, 111, 3, 601],\\n        [24, 10, 105, 2, 1072],\\n        [31, 4, 66, 1, 417],\\n        [56, 3, 60, 1, 36],\\n        [49, 3, 147, 2, 179],\\n    ]\\n)\\ny = np.array(\\n    [\\n        335800.0,\\n        379100.0,\\n        118950.0,\\n        247200.0,\\n        107950.0,\\n        266550.0,\\n        75850.0,\\n        93300.0,\\n        170650.0,\\n        149000.0,\\n    ]\\n)\\n\\n\\ndef hidden_activation(z):\\n    # ReLU activation. fix this!\\n    # ReLU activation function returns either the input value of the function, or zero, whichever is the largest\\n    return np.maximum(0, z)\\n\\n\\ndef output_activation(z):\\n    # identity (linear) activation. fix this!\\n    # linear activation just returns the input as output\\n    return z\\n\\n\\nx_test = [[72, 2, 25, 3, 450], [60, 3, 15, 1, 300], [74, 5, 10, 2, 100]]\\nfor item in x_test:\\n    h1_in = (\\n        np.dot(item, w0) + b0\\n    )  # this calculates the linear combination of inputs and weights. it is missing the bias term, fix it!\\n    h1_out = hidden_activation(h1_in)  # apply activation function\\n\\n    h2_in = (\\n        np.dot(h1_out, w1) + b1\\n    )  # the output of the previous layer is the input for this layer. it is missing the bias term, fix it!\\n    h2_out = hidden_activation(h2_in)\\n\\n    out_in = np.dot(h2_out, w2) + b2\\n    out = output_activation(out_in)\\n    print(out)\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            "
     },
     "metadata": {}
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "w0 = np.array(\n",
    "    [\n",
    "        [1.19627687e01, 2.60163283e-01],\n",
    "        [4.48832507e-01, 4.00666119e-01],\n",
    "        [-2.75768443e-01, 3.43724167e-01],\n",
    "        [2.29138536e01, 3.91783025e-01],\n",
    "        [-1.22397711e-02, -1.03029800e00],\n",
    "    ]\n",
    ")\n",
    "\n",
    "w1 = np.array([[11.5631751, 11.87043684], [-0.85735419, 0.27114237]])\n",
    "\n",
    "w2 = np.array([[11.04122165], [10.44637262]])\n",
    "\n",
    "b0 = np.array([-4.21310294, -0.52664488])\n",
    "b1 = np.array([-4.84067881, -4.53335139])\n",
    "b2 = np.array([-7.52942418])\n",
    "\n",
    "x = np.array(\n",
    "    [\n",
    "        [111, 13, 12, 1, 161],\n",
    "        [125, 13, 66, 1, 468],\n",
    "        [46, 6, 127, 2, 961],\n",
    "        [80, 9, 80, 2, 816],\n",
    "        [33, 10, 18, 2, 297],\n",
    "        [85, 9, 111, 3, 601],\n",
    "        [24, 10, 105, 2, 1072],\n",
    "        [31, 4, 66, 1, 417],\n",
    "        [56, 3, 60, 1, 36],\n",
    "        [49, 3, 147, 2, 179],\n",
    "    ]\n",
    ")\n",
    "y = np.array(\n",
    "    [\n",
    "        335800.0,\n",
    "        379100.0,\n",
    "        118950.0,\n",
    "        247200.0,\n",
    "        107950.0,\n",
    "        266550.0,\n",
    "        75850.0,\n",
    "        93300.0,\n",
    "        170650.0,\n",
    "        149000.0,\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "def hidden_activation(z):\n",
    "    # ReLU activation. fix this!\n",
    "    # ReLU activation function returns either the input value of the function, or zero, whichever is the largest\n",
    "    return np.maximum(0, z)\n",
    "\n",
    "\n",
    "def output_activation(z):\n",
    "    # identity (linear) activation. fix this!\n",
    "    # linear activation just returns the input as output\n",
    "    return z\n",
    "\n",
    "\n",
    "x_test = [[72, 2, 25, 3, 450], [60, 3, 15, 1, 300], [74, 5, 10, 2, 100]]\n",
    "for item in x_test:\n",
    "    h1_in = (\n",
    "        np.dot(item, w0) + b0\n",
    "    )  # this calculates the linear combination of inputs and weights. it is missing the bias term, fix it!\n",
    "    h1_out = hidden_activation(h1_in)  # apply activation function\n",
    "\n",
    "    h2_in = (\n",
    "        np.dot(h1_out, w1) + b1\n",
    "    )  # the output of the previous layer is the input for this layer. it is missing the bias term, fix it!\n",
    "    h2_out = hidden_activation(h2_in)\n",
    "\n",
    "    out_in = np.dot(h2_out, w2) + b2\n",
    "    out = output_activation(out_in)\n",
    "    print(out)"
   ]
  },
  {
   "source": [
    "**What price does the neural network predict for the cabin in question?**\n",
    "- roughly 233000\n",
    "\n",
    "**What type of a machine learning problem is this?**\n",
    "\n",
    "- [ ] unsupervised learning\n",
    "- [x] supervised learning\n",
    "- [ ] reinforcement learning\n",
    "\n",
    "**How can we make sure we are not overfitting the neural network to the data?**\n",
    "\n",
    "- [ ] neural network will always overfit because there are too many parameters for a linear problem like this\n",
    "- [ ] use the full set of cabin data as a training set, and a small subset of it as a testing set\n",
    "- [x] use cross-validation"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Exercise 21: Neural Networks\n",
    "\n",
    "### -- Advanced\n",
    "\n",
    "We have trained a simple neural network with a larger set of cabin price data. The network predicts the price of the cabin based on the attributes of the cabin. The network consists of an input layer with five nodes, a hidden layer with two nodes, a second hidden layer with two nodes, and finally an output layer with a single node. In addition, there is a single bias node for each hidden layer and the output layer.\n",
    "\n",
    "The program below uses the weights of this trained network to perform what is called a forward pass of the neural network. The forward pass is running the input variables through the neural network to obtain output, in this case the price of a cabin of given attributes.\n",
    "\n",
    "The program is incomplete though. The program only does the forward pass up to the first hidden layer and is missing the second hidden layer and the output layer.\n",
    "\n",
    "Modify the program to do a full forward pass and print out the price prediction. To do this, write out the remaining forward pass operations and use the ReLU activation function for the hidden nodes, and a linear (identity) activation for the output node. ReLU activation function returns either the input value of the function, or zero, whichever is the largest, and linear activation just returns the input as output. After these are done, get a prediction for the price of a cabin which is described by the following feature vector `[82, 2, 65, 3, 516]`."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[257136.4]\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.Javascript object>",
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 55;\n                var nbb_unformatted_code = \"import numpy as np\\n\\nw0 = np.array([[ 1.19627687e+01,  2.60163283e-01],\\n               [ 4.48832507e-01,  4.00666119e-01],\\n                   [-2.75768443e-01,  3.43724167e-01],\\n                   [ 2.29138536e+01,  3.91783025e-01],\\n                   [-1.22397711e-02, -1.03029800e+00]])\\n\\nw1 = np.array([[11.5631751 , 11.87043684],\\n                   [-0.85735419,  0.27114237]])\\n\\nw2 = np.array([[11.04122165],\\n                   [10.44637262]])\\n\\nb0 = np.array([-4.21310294, -0.52664488])\\nb1 = np.array([-4.84067881, -4.53335139])\\nb2 = np.array([-7.52942418])\\n\\nx = np.array([[111, 13, 12, 1, 161],\\n                 [125, 13, 66, 1, 468],\\n                 [46, 6, 127, 2, 961],\\n                 [80, 9, 80, 2, 816],\\n                 [33, 10, 18, 2, 297],\\n                 [85, 9, 111, 3, 601],\\n                 [24, 10, 105, 2, 1072],\\n                 [31, 4, 66, 1, 417],\\n                 [56, 3, 60, 1, 36],\\n                 [49, 3, 147, 2, 179]])\\ny = np.array([335800., 379100., 118950., 247200., 107950., 266550.,  75850.,\\n                93300., 170650., 149000.])\\n\\n\\ndef hidden_activation(z):\\n    # ReLU activation. fix this!\\n    return np.maximum(0, z)\\n\\ndef output_activation(z):\\n    # identity (linear) activation. fix this!\\n    return z\\n\\nx_test = [[82, 2, 65, 3, 516]]\\nfor item in x_test:\\n    h1_in = np.dot(item, w0) + b0 # this calculates the linear combination of inputs and weights\\n    h1_out = hidden_activation(h1_in) # apply activation function\\n    \\n    # fill out the missing parts:\\n    # the output of the first hidden layer, h1_out, will need to go through\\n    # the second hidden layer with weights w1 and bias b1\\n    h2_in = np.dot(h1_out, w1) + b1\\n    h2_out = hidden_activation(h2_in) # apply activation function\\n\\n    # and finally to the output layer with weights w2 and bias b2.\\n    # remember correct activations: relu in the hidden layers and linear (identity) in the output\\n    out_in = np.dot(h2_out, w2) + b2\\n    out = output_activation(out_in)\\n    print(out)\";\n                var nbb_formatted_code = \"import numpy as np\\n\\nw0 = np.array(\\n    [\\n        [1.19627687e01, 2.60163283e-01],\\n        [4.48832507e-01, 4.00666119e-01],\\n        [-2.75768443e-01, 3.43724167e-01],\\n        [2.29138536e01, 3.91783025e-01],\\n        [-1.22397711e-02, -1.03029800e00],\\n    ]\\n)\\n\\nw1 = np.array([[11.5631751, 11.87043684], [-0.85735419, 0.27114237]])\\n\\nw2 = np.array([[11.04122165], [10.44637262]])\\n\\nb0 = np.array([-4.21310294, -0.52664488])\\nb1 = np.array([-4.84067881, -4.53335139])\\nb2 = np.array([-7.52942418])\\n\\nx = np.array(\\n    [\\n        [111, 13, 12, 1, 161],\\n        [125, 13, 66, 1, 468],\\n        [46, 6, 127, 2, 961],\\n        [80, 9, 80, 2, 816],\\n        [33, 10, 18, 2, 297],\\n        [85, 9, 111, 3, 601],\\n        [24, 10, 105, 2, 1072],\\n        [31, 4, 66, 1, 417],\\n        [56, 3, 60, 1, 36],\\n        [49, 3, 147, 2, 179],\\n    ]\\n)\\ny = np.array(\\n    [\\n        335800.0,\\n        379100.0,\\n        118950.0,\\n        247200.0,\\n        107950.0,\\n        266550.0,\\n        75850.0,\\n        93300.0,\\n        170650.0,\\n        149000.0,\\n    ]\\n)\\n\\n\\ndef hidden_activation(z):\\n    # ReLU activation. fix this!\\n    return np.maximum(0, z)\\n\\n\\ndef output_activation(z):\\n    # identity (linear) activation. fix this!\\n    return z\\n\\n\\nx_test = [[82, 2, 65, 3, 516]]\\nfor item in x_test:\\n    h1_in = (\\n        np.dot(item, w0) + b0\\n    )  # this calculates the linear combination of inputs and weights\\n    h1_out = hidden_activation(h1_in)  # apply activation function\\n\\n    # fill out the missing parts:\\n    # the output of the first hidden layer, h1_out, will need to go through\\n    # the second hidden layer with weights w1 and bias b1\\n    h2_in = np.dot(h1_out, w1) + b1\\n    h2_out = hidden_activation(h2_in)  # apply activation function\\n\\n    # and finally to the output layer with weights w2 and bias b2.\\n    # remember correct activations: relu in the hidden layers and linear (identity) in the output\\n    out_in = np.dot(h2_out, w2) + b2\\n    out = output_activation(out_in)\\n    print(out)\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            "
     },
     "metadata": {}
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "w0 = np.array(\n",
    "    [\n",
    "        [1.19627687e01, 2.60163283e-01],\n",
    "        [4.48832507e-01, 4.00666119e-01],\n",
    "        [-2.75768443e-01, 3.43724167e-01],\n",
    "        [2.29138536e01, 3.91783025e-01],\n",
    "        [-1.22397711e-02, -1.03029800e00],\n",
    "    ]\n",
    ")\n",
    "\n",
    "w1 = np.array([[11.5631751, 11.87043684], [-0.85735419, 0.27114237]])\n",
    "\n",
    "w2 = np.array([[11.04122165], [10.44637262]])\n",
    "\n",
    "b0 = np.array([-4.21310294, -0.52664488])\n",
    "b1 = np.array([-4.84067881, -4.53335139])\n",
    "b2 = np.array([-7.52942418])\n",
    "\n",
    "x = np.array(\n",
    "    [\n",
    "        [111, 13, 12, 1, 161],\n",
    "        [125, 13, 66, 1, 468],\n",
    "        [46, 6, 127, 2, 961],\n",
    "        [80, 9, 80, 2, 816],\n",
    "        [33, 10, 18, 2, 297],\n",
    "        [85, 9, 111, 3, 601],\n",
    "        [24, 10, 105, 2, 1072],\n",
    "        [31, 4, 66, 1, 417],\n",
    "        [56, 3, 60, 1, 36],\n",
    "        [49, 3, 147, 2, 179],\n",
    "    ]\n",
    ")\n",
    "y = np.array(\n",
    "    [\n",
    "        335800.0,\n",
    "        379100.0,\n",
    "        118950.0,\n",
    "        247200.0,\n",
    "        107950.0,\n",
    "        266550.0,\n",
    "        75850.0,\n",
    "        93300.0,\n",
    "        170650.0,\n",
    "        149000.0,\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "def hidden_activation(z):\n",
    "    # ReLU activation. fix this!\n",
    "    return np.maximum(0, z)\n",
    "\n",
    "\n",
    "def output_activation(z):\n",
    "    # identity (linear) activation. fix this!\n",
    "    return z\n",
    "\n",
    "\n",
    "x_test = [[82, 2, 65, 3, 516]]\n",
    "for item in x_test:\n",
    "    h1_in = (\n",
    "        np.dot(item, w0) + b0\n",
    "    )  # this calculates the linear combination of inputs and weights\n",
    "    h1_out = hidden_activation(h1_in)  # apply activation function\n",
    "\n",
    "    # fill out the missing parts:\n",
    "    # the output of the first hidden layer, h1_out, will need to go through\n",
    "    # the second hidden layer with weights w1 and bias b1\n",
    "    h2_in = np.dot(h1_out, w1) + b1\n",
    "    h2_out = hidden_activation(h2_in)  # apply activation function\n",
    "\n",
    "    # and finally to the output layer with weights w2 and bias b2.\n",
    "    # remember correct activations: relu in the hidden layers and linear (identity) in the output\n",
    "    out_in = np.dot(h2_out, w2) + b2\n",
    "    out = output_activation(out_in)\n",
    "    print(out)"
   ]
  },
  {
   "source": [
    "# Conclusion"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Your AI idea\n",
    "\n",
    "The optional final task of this course consists of your own AI idea. We are not giving you a made-up problem to solve. Instead, we want to hear what kind of a problem you'd like to solve using AI – and how.\n",
    "\n",
    "To make it easier for you, we’re proposing you structure the project description around a list of topics. Once you have written down a few thoughts about each of these topics, you already have enough material to submit your project! If you’re up to it, you can also expand this into a working demo or prototype with code and data."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### The topics we’ll ask you to elaborate are:\n",
    "\n",
    "1. Your idea in a nutshell: Name your project and prepare to describe it briefly.\n",
    "\n",
    "2. Background: What is the problem your idea will solve? How common or frequent is this problem? What is your personal motivation? Why is this topic important or interesting?\n",
    "\n",
    "3. Data and AI techniques: What data sources does your project depend on? Almost all AI solutions depend on some data. The availability and quality of the data are essential. Which AI techniques do you think will be helpful? Depending on whether you've been doing the programming exercises or not, you may choose to include a concrete demo implemented by coding, using some actual data!\n",
    "\n",
    "4. How is it used: What is the context in which your solution is used, and by whom? Who are the people affected by it? It’s important to appreciate the viewpoints of all those affected.\n",
    "\n",
    "5. Challenges: What does your project not solve? It’s important to understand that any technological solution will have its limitations.\n",
    "\n",
    "6. What next: How could your project grow and become something even more?\n",
    "\n",
    "7. Acknowledgments: If you’re using open source code or documents in your project, make sure you give credit to the creators. Mention your sources of inspiration, too."
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}